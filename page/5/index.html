<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Code Dragon</title><meta property="og:type" content="blog"><meta property="og:title" content="Code Dragon"><meta property="og:url" content="https://sxlong0205.github.io/"><meta property="og:site_name" content="Code Dragon"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://sxlong0205.github.io/img/og_image.png"><meta property="article:author" content="Code Dragon"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://sxlong0205.github.io"},"headline":"Code Dragon","image":["https://sxlong0205.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Code Dragon"},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/github.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Code Dragon" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-28T10:20:09.000Z" title="2020-12-28T10:20:09.000Z">2020-12-28</time><span class="level-item">28 minutes read (About 4155 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/28/%E8%AE%BA%E6%96%87/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks/">论文/Sequence to Sequence Learning with Neural Networks</a></h1><div class="content"><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>深度神经网络（DNN）是一种强大的模型，在困难的学习任务中取得了卓越的性能。尽管 DNNs 在有大的标记训练集时工作得很好，但它们不能用于将序列映射到序列。在本文中，我们提出了一种端到端学习的一般方法。该方法利用多层长短期记忆（LSTM）将输入序列映射到一个固定维数的向量上，然后用另一个深层 LSTM 对目标序列进行解码。我们的主要结果是，在 WMT’14 数据集的英法翻译任务中，LSTM 产生的翻译在整个测试集上达到了34.8 的 BLEU 分数，其中 LSTM 的 BLEU 分数在词汇表外单词上受到惩罚。此外，LSTM 在长句方面没有困难。相比之下，基于短语的 SMT 系统在同一数据集上的 BLEU 得分为 33.3。当我们使用 LSTM 重新评估上述 SMT 系统产生的 1000 个假设时，其 BLEU 得分增加到 36.5，这与之前在这项任务上的最佳结果接近。LSTM 还学习对词序敏感、对主动语态和被动语态相对不变的敏感短语和句子表征。最后，我们发现颠倒所有源句子（而不是目标句子）中的单词顺序可以显著提高 LSTM 的性能，因为这样做会在源句和目标句之间引入许多短期依赖，从而使优化问题变得更容易。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>深度神经网络（DNNs）是一种非常强大的机器学习模型，在语音识别和视觉对象识别等难题上实现了卓越的性能。DNNs 之所以强大，是因为它们可以在不多的步骤内执行任意并行计算。DNNs 的强大威力的一个令人惊讶的例子是它们仅使用 2 个二次大小的隐藏层对 N 个比特数进行排序。因此，虽然神经网络与传统的统计模型有关，但它们学习复杂的计算。此外，只要标记的训练集有足够的信息来指定网络的参数，就可以用监督反向传播来训练大型 DNN。因此，如果存在一个获得良好结果的大型 DNN 的参数设置（例如，因为人类可以非常快速地解决任务），监督反向传播将找到这些参数并解决问题。</p>
<p>尽管 DNNs 具有灵活性和强大的功能，但它只能应用于输入和目标可以用固定维数的向量进行合理编码的问题。这是一个重要的限制，因为许多重要问题最好用长度未知的序列来表示。例如，语音识别和机器翻译都是顺序问题。同样，问题回答也可以看作是将代表问题的单词序列映射到代表答案的单词序列。因此，很明显，学习将序列映射到序列的独立于领域的方法是有用的。</p>
<p>序列对 DNNs 提出了挑战，因为它们要求输入和输出的维数是已知的和固定的。在本文中，我们证明了长-短期记忆（LSTM）体系结构的直接应用可以解决一般的序列间问题。其思想是使用一个 LSTM 读取输入序列，每次一个时间步，以获得大的固定维向量表示，然后使用另一个 LSTM 从该向量中提取输出序列（图1）。第二个 LSTM 本质上是一个递归神经网络语言模型，只是它是以输入序列为条件的。LSTM 能够成功地学习具有长时间依赖关系的数据，这使得它成为该应用的自然选择，因为输入和相应输出之间存在相当大的时间延迟（图1）。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228172228.png" alt="image-20201228172228220"></p>
<p>已经进行了许多相关尝试，以解决一般序列与神经网络的序列学习问题。我们的方法与最早将整个输入句子映射到向量的 Kalchbrenner 和 Blunsom 密切相关，并且与 Cho 等人的方法有关。尽管后者仅用于记录基于短语的系统所产生的假设。Graves 引入了一种新颖的可微分注意力机制，该机制使神经网络能够专注于其输入的不同部分，并且 Bahdanau 等人成功地将该思想的一种优雅变体应用于机器翻译。Connectionist Sequence Classification 是另一种流行的技术，可以利用神经网络来映射端到端，但是它假定输入和输出之间是单调对齐</p>
<p>这项工作的主要结果如下。在 WMT’14 英法翻译任务上，我们利用了一个简单的从左到右的波束搜索解码器，从整合 5 层的深度 LSTMs（每个 384M 参数和 8000 个维度状态）直接提取翻译，获得了 34.81分的 BLEU 分数。这是迄今为止用大型神经网络进行直接翻译所取得的最佳结果。作为比较，该数据集上 SMT 基线的 BLEU 得分为33.30。BLEU 得分为 34.81，是由词汇量为 80k 的 LSTM 获得的，因此每当参考译文中包含 80k 单词以外的单词时，BLEU 分数就会被扣分。这一结果表明，相对未优化的小词汇量神经网络结构比基于短语的 SMT 系统有很大的改进空间。</p>
<p>最后，我们使用 LSTM 对同一任务中公开的 1000 个 SMT 基线最佳列表进行了重新筛选。通过这样做，我们得到了 36.5 的 BLEU 分数，这使基线提高了 3.2 个 BLEU 点，并且接近之前在这项任务中发表的最佳结果（37.0）。</p>
<p>令人惊讶的是，尽管最近有其他研究人员在相关架构方面的经验，LSTM 并没有在很长的句子中受到影响。我们之所以能在长句上表现出色，是因为在训练和测试集中，我们颠倒了源句中单词的顺序，而不是目标句。通过这样做，我们引入了许多短期依赖关系，使优化问题更加简单。因此，SGD 可以学习没有长句问题的 LSTMs。这项工作的主要技术贡献之一就是在源句中颠倒单词的简单技巧。</p>
<p>LSTM 的一个有用的特性是它可以学习将可变长度的输入语句映射为固定维的向量表示。考虑到译文往往是源句的释义，翻译目标鼓励第一语言学习者寻找能够捕捉其意义的句子表示法，因为意义相似的句子彼此接近，而不同的两个句子的意义却相距甚远。一个定性的评估支持了这一说法，表明我们的模型能够感知词序，并且对主动语态和被动语态具有相当的不变性。</p>
<h2 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h2><p>递归神经网络（RNN）是前馈神经网络对序列的自然推广。给定输入序列$(x_1…x_T)$，标准 RNN 通过迭代以下方程计算输出序列$(y_1…y_T)$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_t &= sigm(W^{hx}x_t + W^{hh}h_{t-1}) \\
y_t &= W^{yh}h_t
\end{aligned}</script><p>只要提前知道输入和输出之间是对齐的，RNN 就可以很容易地将序列映射到序列。然而，如何将 RNN 应用于输入和输出序列长度不同、关系复杂且非单调的问题还不明确。</p>
<p>一般序列学习最简单的策略是使用一个 RNN 将输入序列映射到一个固定大小的向量，然后用另一个 RNN 将向量映射到目标序列（Cho 等人也采用了这种方法）。虽然原则上可以工作，因为 RNN 提供了所有相关信息，但由于产生的长期依赖性很难训练 RNNs（图1）。然而，众所周知，长-短期记忆（LSTM）可以学习具有长范围时间依赖性的问题，因此 LSTM 可能在这种情况下取得成功。</p>
<p>LSTM 的目标是估计条件概率$ p（y_1…y_{T′}| x_1…x_T）$其中$(x_1…x_T)$是一个输入序列，$(y_1…y_T)$是其相应的输出序列，其长度 T′ 可能与 T 不同。LSTM 通过首先获得由 LSTM 的最后一个隐藏状态给出的输入序列$(x_1…x_T)$，然后用一个标准的 LSTM-LM 公式计算 $(y_1…y_T)$的概率，其初始隐藏态设为$(x_1…x_T)$的表示$v$：</p>
<script type="math/tex; mode=display">
p（y_1...y_{T′}| x_1...x_T） = \prod_{t-1}^{T'}p(y_t|v,y_1...y_{t-1})</script><p>在这个方程中，每个$p(y_t|v,y_1…y_{t-1})$在词汇表中的所有单词上用 softmax 表示分布。我们使用 Graves 中的 LSTM 公式。注意我们要求每个句子以一个特殊的句子结尾符号“<EOS>”，这使得模型能够定义所有可能长度的序列的分布。总体方案如图 1 所示，其中所示的 LSTM 计算“A”、“B”、“C”和“<EOS>”的表示，然后使用该表示法计算“W”、“X”、“Y”、“Z”和“<EOS>”的概率。</p>
<p>我们的实际模型在三个重要方面与上述描述不同。首先，我们使用了两种不同的 LSTM：一种用于输入序列，另一种用于输出序列，因为这样做会以可忽略的计算成本增加数字模型参数，并使得同时在多个语言对上训练 LSTM 变得很自然。第二，我们发现深层 LSTMs 的表现明显优于浅层 LSTMs，因此我们选择了四层的 LSTM。第三，我们发现倒序输入句非常有价值。例如，LSTM 不是将句子 a，b，c 映射到句子 α，β，γ，而是要求 LSTM 将 c，b，a 映射到 α，β，γ，其中 α，β，γ 是 a，b，c 的翻译。这样一来，a 非常接近 α，b 相当接近 β，以此类推，这一事实使得 SGD 很容易在输入和输出之间建立联系。我们发现这种简单的数据转换可以大大提高 LSTM 的性能。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>神经网络在机器翻译中的应用有大量的工作要做。到目前为止，将 RNN 语言模型（RNNLM）或一个前馈神经网络语言模型应用到机器翻译任务中，通过重新筛选强机器翻译基线的 n 最佳列表，可以提高翻译质量。</p>
<p>最近，研究人员开始研究如何将源语言的信息包含到 NNLM 中。这项工作的例子包括 Auli 等人，他们将 NNLM 与输入语句的主题模型相结合，从而提高了重新排序的性能。Devlin 等人采用了类似的方法，但他们将 NNLM 合并到 MT 系统的解码器中，并使用解码器的对齐信息为 NNLM 提供输入语句中最有用的单词。他们的方法非常成功，并且在基线上取得了很大的改进。</p>
<p>我们的工作与Kalchbrenner和Blunsom[18]密切相关，他们第一个将输入句子映射到向量，然后再映射回句子，尽管他们使用卷积神经网络将句子映射到向量，这会失去单词的顺序。与这项工作类似，Cho等人[5]使用类似LSTM的RNN架构将句子映射到向量和反向，尽管他们的主要关注点是将他们的神经网络集成到SMT系统中。Bahdanau等人[2]还尝试用一种神经网络直接翻译，这种神经网络使用注意机制来克服Cho等人[5]在长句上表现不佳的问题，并取得了令人鼓舞的结果。同样，Pouget Abadie等人[26]试图解决Cho等人[5]的记忆问题，通过翻译源句子的片段来产生流畅的翻译，这类似于基于短语的方法。我们怀疑，他们可以通过简单地训练他们的网络来获得类似的改进。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>在这项工作中，我们证明了在大规模机器翻译任务中，词汇量有限且几乎没有问题结构假设的大型深层 LSTM 可以比基于 SMT 的标准系统表现更好。我们简单的基于 LSTM 的 MT 方法的成功表明，只要它们有足够的训练数据，它就可以很好地解决许多其他序列学习问题。</p>
<p>我们惊讶于通过颠倒源句中的单词所获得的改进程度。我们的结论是，重要的是找到一个问题编码有最大数量的短期依赖，因为他们使学习问题更简单。特别是，虽然我们无法在非反转翻译问题上训练标准 RNN（如图1所示），但我们认为，当源语句被颠倒时，标准 RNN 应该是容易训练的（尽管我们没有通过实验验证）。</p>
<p>我们还对 LSTM 正确翻译很长句子的能力感到惊讶。我们最初确信，由于记忆有限，LSTM 在长句上会失败，而其他研究人员报告说，使用类似于我们的模型的长句表现不佳。然而，在反向数据集上训练的 LSTM 在翻译长句时几乎没有困难。</p>
<p>最重要的是，我们证明了一个简单、直截了当且相对未优化的方法可以优于 SMT 系统，因此进一步的工作可能会探索更高的翻译精度。这些结果表明，我们的方法很可能在其他具有挑战性的序列对序列问题上做得很好。</p>
<p>我们的工作与 Kalchbrenner 和 Blunsom 密切相关，他们第一个将输入句子映射到向量，然后再映射回句子，尽管他们使用卷积神经网络将句子映射到向量，这会失去单词的顺序。与这项工作类似，Cho 等人使用类似 LSTM 的 RNN 架构将句子映射到向量和反向，尽管他们的主要关注点是将他们的神经网络集成到 SMT 系统中。Bahdanau 等人还尝试用一种神经网络直接翻译，这种神经网络使用注意力机制来克服 Cho 等人在长句上表现不佳的问题，并取得了令人鼓舞的结果。同样，Pouget Abadie 等人试图解决 Cho 等人的记忆问题，通过翻译源句子的片段来产生流畅的翻译，这类似于基于短语的方法。我们怀疑，他们可以通过简单地训练他们的网络来获得类似的改进。</p>
<p>端到端训练也是 Hermann 等人的重点，他们的模型通过前馈网络表示输入和输出，并将它们映射到空间中的相似点。然而，他们的方法不能直接生成翻译：为了得到翻译，他们需要在预先计算的句子数据库中查找最近的向量，或者重新扫描句子。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-28T08:36:40.000Z" title="2020-12-28T08:36:40.000Z">2020-12-28</time><span class="level-item">2 hours read (About 21230 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/28/%E8%AE%BA%E6%96%87/Deep%20Learning%20Based%20Text%20Classification%EF%BC%9AA%20Comprehensive%20Review/">论文/Deep Learning Based Text Classification：A Comprehensive Review</a></h1><div class="content"><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在繁多的文本分类任务中，基于深度学习的模型已经超过了基于传统的机器学习的方法，包括舆情分析、新闻分类、问题回答和自然语言推理。在本文中，我们对近年来的150多种基于深度学习的文本分类模型进行了全面回顾，并讨论了它们的技术贡献，相似性和优势。同时我们提供了四十多种广泛应用于文本分类领域的数据集的摘要。最后，我们定量分析了不同深度学习模式在流行基准上的表现，并探讨了未来的研究方向。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Text Classification 也被称为 Text Categorization，是自然语言处理领域的一种经典的问题，旨在为文本单元分配标签或者标记，例如句子、疑问句、段落和文档。文本分类具有广泛的应用，包括问答、垃圾邮件检测、舆情分析、行文分类、用户意图分类、内容检测。文本数据可以来自于不同的领域，包括网页数据、电子邮件、聊天内容、社会媒体、票据、保险理赔、用户评论和客户服务的问题和回答等等。文本来源极为丰富，但是由于其非结构化的特性，从中提取有用信息是十分具有挑战性和耗时的。</p>
<p>文本分类可以通过手工标注或者自动标记来执行。随着工业领域文本数据的规模极速上涨，自动文本分类变得越来越重要。自动文本分类的方法可以分为两类：</p>
<ul>
<li>基于规则的方法</li>
<li>基于机器学习的方法</li>
</ul>
<p>基于规则的方法使用一组预定义的规则将文本分为不同的类别，并且需要对领域有深入的了解。另一方面，机器学习的方法基于对以往数据的观察来进行分类。使用预训练样本作为训练集，机器学习算法可以学习出文本和标签的内在关系。</p>
<p>机器学习模型在近几年吸引了众多的关注。大多数经典的机器学习模型遵循流行的两步法，第一步从文档中提取一些手工特性（或者其他一些文本单元），第二步将这些特性喂入分类其中进行预测。一些流行的手工特性包括词袋模型（BoW）和它的延伸模型。流行的分类算法有 Naive Bayes 模型，SVM，Hiden Markov Model， Gradient Boosting Trees 和 Random Forest。两步法中有几点限制。比如，依靠手工特性意味着为了获取好的表现，需要乏味的特征工程和分析。此外，设计特征对领域知识的强依赖，使得方法难以推广到新任务。最后，因为特征都是预训练的，所以这些模型不能够充分使用大量训练数据。</p>
<p>神经方法被提出用于解决上述手工特性带来的限制。这些方法的核心就是机器学习的嵌入模型，该模型将文本映射到低维连续特征向量，因此不需要手工特征。最早的嵌入模型叫做 Latent Semantic Analysis（LSA），是 Dumais 和她的同事在 1989 年提出的。LSA 是一个参数少于一百万，在二十万词汇上训练出来的线性模型。在 2001 年，Bengio 的团队提出了第一个基于前馈神经网络的神经语言模型，该模型基于 1400 万词汇训练。然而，这些早期的嵌入模型在使用手工特征上相较于经典模型表现平平，因此也没有被广泛的使用。这种情况出现转变是通过更大数量的训练数据训练出更大的嵌入模型。在 2013 年，Google 开发了一系列 wrod2vec 模型，这些模型基于 60 亿单词训练，并立即在许多 NLP 任务中流行。在 2017 年，来自 AI2 和华盛顿大学的团队开发了一种基于上下文的嵌入模型，该模型基于三层 LSTM 和 93M 的参数在 1B 的单词上训练得到。这个模型被称为 ELMo，比 word2vec 表现更好，因为其捕获了上下文信息。在 2018 年，OpenAI 开始使用 Transformer 建立嵌入模型，这是 Google 开发的一种新的神经网络架构。Transformer 完全依靠于 Attention 机制，这大大提高了大规模模型在 TPU 上训练的效率，他们的一个模型被称为 GPT，现已广泛应用于文本生成任务。同年，Google 开发了基于双向 Transformer 的 BERT。BERT 由 340M 参数组成，经过 33 亿单词的训练，当前现在最先进的嵌入模型。使用更大的模型和更多的训练参数的趋势还在持续。在这篇文章出版的时候，OpenAI 最新的 GPT-3 模型包含了 1700 亿参数，Google 的 GShard 包含了 6000 万参数。</p>
<p>尽管这些巨大的模型在各种 NLP 任务上显示出了非常出色的性能，但一些研究人员认为它们并不真正理解语言，因此对于许多关键任务领域而言不够健壮。近期，人们越来越有兴趣探索神经符号混合模型，以解决神经模型的一些基本局限性，例如缺乏基础，无法执行符号推理，无法解释等等。这些工作尽管很重要，但是超出了本文论述范围。</p>
<p>虽然有很多关于文本分离方法和应用的好的评论和书籍，但是这项调研的独特之处在于它提供了在过去的六年中针对各种文本分类任务开发的 150 多种深度学习模型的综述，包括情感分析、新闻分类、话题分类、问答系统和自然语言推理。特别的，我们根据这些神经网络架构将这些工作分为几类，比如基于 RNN 的模型，基于 CNN 的模型、基于注意力机制模型、基于 Transformer 模型、基于 Capsule Nets 模型等等。本文的贡献如下：</p>
<ul>
<li>对文本分类领域中提出的超过 150 种深度学习模型进行了详细的概述</li>
<li>回顾了 40 多种流行的文本分类数据集</li>
<li>根据 16 种流行的基准测试对选定的一组深度学习模型的性能进行了定量的分析</li>
<li>讨论了留存的挑战和未来的方向</li>
</ul>
<h3 id="Text-Classification-Tasks"><a href="#Text-Classification-Tasks" class="headerlink" title="Text Classification Tasks"></a>Text Classification Tasks</h3><p>文本分类（TC）是将文本（例如推文、新闻文章、客户评论）分组的过程。典型的 TC 任务包括情感分析、新闻分类和话题分类。最近，研究人员表明，通过允许 DL- based 文本分类器将一对文本作为输入，将许多 Natural Language Understanding 任务（例如提取问题回答、自然语言推断）转换为 TC 任务是有效的。这一部分介绍了五个 TC 任务，包括三个典型的 TC 任务和两个在许多最近 DL 研究中被转化为 TC 的 NLU 任务。</p>
<ul>
<li>Sentiment Analysis</li>
</ul>
<p>这是一种在文本数据中分析人们观点的任务（比如产品评论、电影评论和推文），并提取他们的正负情绪和观点。这种任务能够被转换为二分类或者多分类问题。二分类情感分析是将文本分类正面和负面两种类型，而多分类问题则专注于将数据分类为细粒度标签或多级强度。</p>
<ul>
<li>News Categorization</li>
</ul>
<p>新闻内容是最重要的信息来源之一。一个新闻分类系统通过识别新出现的新闻话题或根据用户兴趣推荐相关新闻，来帮助用户获取实时的兴趣信息。</p>
<ul>
<li>Topic Analysis</li>
</ul>
<p>这类任务，作为人们熟知的话题分类，目标是识别一段文本的主题（比如了解产品评论是关于“客户支持”还是“易用性”）。</p>
<ul>
<li>Question Answering</li>
</ul>
<p>有两种类型的 QA 系统：提取式和生成式。提取式 QA 系统是一种文本分类任务：给定一个问题和一组候选答案（比如 SQuAD 中文本的范围），区分每一个候选答案正确与否。生成式 QA 是一种文本生成任务，因为它需要即时生成答案。本文仅讨论提取式 QA。</p>
<ul>
<li>Natural Language Inference（NLI）</li>
</ul>
<p>NLI，也被称为 Recognizing Textual Entailment（RTE），预测是否可以从另一文本推断出文本的含义。一个 NLI 系统需要给一对文本单元分配一个标签，例如包含、矛盾和中性。释义是 NLI 的一种广义形式，也称为文本比较，即衡量句子语义相似度，表明一个句子与另一个句子相似的可能性。</p>
<h3 id="Paper-Structure"><a href="#Paper-Structure" class="headerlink" title="Paper Structure"></a>Paper Structure</h3><p>本文其余结构如下：Section 2 全面概述了 150 多种基于深度学习的文本分类模型。Section 3 介绍了使用神经网络模型构建文本分类器的方法。Section 4 回顾了一些最受欢迎的文本分类数据集。Section 5 提供了基于 16 个基准测试的一组深度学习模型的定量性能分析。Section 6 讨论了基于深度学习的文本分类方法的主要挑战和未来方向。Section 7 总结了全文。</p>
<h2 id="Deep-Learning-Models-For-Text-Classification"><a href="#Deep-Learning-Models-For-Text-Classification" class="headerlink" title="Deep Learning Models For Text Classification"></a>Deep Learning Models For Text Classification</h2><p>在本节种，我们回顾了针对各种文本分类问题提出的 150 多种深度学习框架。为了方便介绍，我们根据这些模型的贡献分为以下几类：</p>
<ul>
<li>基于前馈神经网络的模型，这种模型将文本看作是词袋模型</li>
<li>基于 RNN 的模型，这种模型将文本看作是单词序列，并且试图捕获单词的依赖和文本的结构</li>
<li>基于 CNN 的模型，这种模型经过训练可识别文本的样式（例如关键短语）以进行分类</li>
<li>Capsule Networks，这种模型解决了 CNN 池化操作所打来的信息丢失问题，最近已应用于文本分类</li>
<li>Attention 机制，这种模型有效的验证了文本中单词的关系，并在发展深度学习模型中非常有用</li>
<li>Memory- Augmented Networks，这种模型将神经网络与外部存储器结合在一起，模型可从中读取和写入数据</li>
<li>Graph Neural Networks，这种模型旨在捕获自然语言的内部图结构，例如句法和语义解析树</li>
<li>Siamese Neural Networks，用于解决文本匹配，一种特殊的文本分类案例。</li>
<li>混合模型，这种模型结合了 Attention 机制、RNN、CNN等，来捕获句子和文档局部和全局的特征。</li>
<li>Transformers，与 RNN 相比，能够实现更多的并行化，从而可以使用 GPU 集群有效地（预）训练非常大的语言模型。</li>
<li>最后，我们回顾了有监督学习之外的建模技术，包括使用自动编码器和对抗训练的无监督学习，以及强化学习。</li>
</ul>
<p>希望读者对基本的深度学习模型有一定的了解，以理解本节内容。有关基于深度学习框架和模型的更多详细信息，请阅读 Goodfellow 等人的深度学习教科书。</p>
<h3 id="Feed-Forward-Neural-Networks"><a href="#Feed-Forward-Neural-Networks" class="headerlink" title="Feed-Forward Neural Networks"></a>Feed-Forward Neural Networks</h3><p>前馈神经网络是用于文本表示的最简单的深度学习模型之一。然而，这些模型在许多文本分类的基准测试中有着很好的表现。这些模型将文本看作是词袋模型。对于每一个单词，模型通过使用词嵌入学习出一个向量表示，例如 word2vec 或者 Glove，用词向量的和或者平均表示文本，使向量通过一层或者多层的前馈网络，称为多层感知机，然后使用分类器（例如 Logistic Regression、Naive Bayes 或者 SVM）对最终层的表示形式进行分类。这些模型的一个例子是 Deep Average Network，它的结构如下图所示。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/mac/20201223172526.png" alt="image-20201223172526642"></p>
<p>尽管非常简单，DAN 表现优于其他更莫咋的模型，这些模型旨在显式学习文本的组成。例如，DAN 在具有较高语法差异的数据集上优于语言模型。Joulin 等，提出了一个称为 fastText 的简单且有效的文本分类器，和 DAN 一样，fastText 将文本看作是一个词袋模型。不同的是，fastText 使用一种 n-grams 的词袋作为附加的属性以捕获局部单词的顺序信息。事实证明，在实践中非常有效，同时可以获得与显式使用单词顺序方法相当的结果。</p>
<p>Le 和 Mikolov 提出了 doc2vec，这种模型使用无监督算法来学习一个可变长度文本（如句子、段落和文档）的定长特征表示。如下图所示，doc2vec 的结构和连续词袋模型相似。它们之间唯一个不同是附加的段落标记通过矩阵 D 映射到段落向量。在 doc2vec 中，此向量与三个单词的撒谎国内下文的联合或者平均用力预测第四个单词。段落向量代表当前上下文中的缺失信息，可以作为段落话题的记忆。在训练之后，段落向量作为该段落的特征（例如代替或作为 BoW 的特征），并喂入分类器用于预测。 Doc2Vec 在发布的时候，在一些文本分类和舆情分析任务上去了的最新的结果。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/mac/20201223172507.png" alt="image-20201223172507855"></p>
<h3 id="RNN-Based-Models"><a href="#RNN-Based-Models" class="headerlink" title="RNN-Based Models"></a>RNN-Based Models</h3><p>基于 RNN 的模型将文本看作是单词序列，并企图捕获单词的依赖和文本结构来进行文本分类。然而，普通 RNN 模型并没有很好的表现，并且通常表现不如前馈神经网络。在 RNNs 许多变种当中，Long Short-Term Memory（LSTM） 是最流行的体系结构，旨在更好地捕获长期依赖关系。LSTM 通过引入一个记忆单元来记住任意时间间隔的值以及三个门（输入门、输出门和以往们）来调节信息的流动方向，从而解决梯度爆炸或者消失的问题。现如今已有工作通过捕获更加丰富的信息来提高 RNN 和 LSTM 模型在文本分类中的表现，比如自然语言的树形结构，文本中长跨度单词之间的关系，文档主题等。</p>
<p>Tai 等，已经开发了一种 Tree-LSTM 模型，将 LSTM 推广到树结构的网络类型学，来学习富语义表示。作者认为 Tree-LSTM 要比链式的 LSTM 在 NLP 任务中的表现更好，因为自然语言具有句法属性，可以自然的地将单词和短语组合在一起。他们通过两个任务验证 Tree-LSTM 的有效性：情感分类和预测两个句子之间的语义相关性。该模型结构如下图所示。Zhu 等人，也将链式 LSTM 扩展到树形结构，使用记忆单元在递归过程中来存储多个子单元或者多个后代单元的历史信息。他们认为，新模型提供了一种原则上的方式考虑层次结构（例如语言或者图像解析结构）上的远程交互。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/mac/20201223174025.png" alt="image-20201223174025239"></p>
<p>为了对机器阅读进行长跨度单词关系的建模，Cheng 等提出用记忆网络代替单个记忆单元来增强 LSTM 结构。这可以在神经注意力递归期间启用自适应单元，从而提供一种弱化标记之间关系的方法。该模型在语言模型、情感分析和 NLI 上取得了可喜的结果。</p>
<p>Multi-Timescale LSTM 神经网络通过捕获不同时域的有价值信息来对对长文本（例如句子和文档）建模。MT-LSTM 将标准的 LSTM 隐藏层分为几组。每一组在不同的时间被激活更新。因此，MT-LSTM 能够对非常长的文本进行建模。MT-LSTM 在文本分类方面优于一组基准测试，包括基于 LSTM 和 RNN 模型。</p>
<p>RNN 擅长捕获单词序列的局部结构，但是记忆长范围的依赖关系会遭遇困难。相反的，潜在话题模型能够捕获文档的全局语义结构，但是不考虑单词的顺序。Bieng 等，提出了一个 TopicRNN 模型来整合 RNN 和 潜在话题模型的优点。它使用 RNNs 捕获局部依赖，使用潜在话题捕获全局依赖关系。TopicRNN 在情感分析领域超过了 RNNs 的表现。</p>
<p>另一种有趣的基于 RNN 的模型。Liu 等，使用多任务学习来训练 RNN，以利用来自多个相关任务的标记训练数据。Johnson 和 Rie 探索了使用 LSTM 的文本区域嵌入方法。Zhou 等整合了双向 LSTM（Bi-LSTM），通过两纬度的最大池化来捕获文本特征。Wang 等提出了使用 Bi-LSTM 模型生成多个位置句子表示的语义匹配。</p>
<p>值得注意的是，RNNs 属于 DNNs 的广泛类别，称为递归神经网络。递归神经网络在结构输入上递归地应用相同的权重集，以在可变大小的输入上产生结构化的预测或矢量表示。RNN 是具有线性链结构的输入的递归神经网络，而递归神经网络在分层结构上运行，例如解析自然语言句子的树形结构，将子表示和父表示结合。RNNs 是文本分类领域最流行的递归神经网络，因为它们高效且方便使用——它们将文本看作是标记的序列，且不需要附加的结构标签，例如分析树。</p>
<h3 id="CNN-Based-Models"><a href="#CNN-Based-Models" class="headerlink" title="CNN-Based Models"></a>CNN-Based Models</h3><p>RNNs 用来识别跨时间的模型，而 CNNs 旨在学习出跨空间的模型。RNNs 在需要理解长范围的语义的 NLP 任务中表现的效果非常好，比如词性标注或者 QA 系统，而当当检测局部和位置不变的模式很重要时，CNNs 可以很好地工作。这些模式可能是表达特定情绪的关键短语，如“我喜欢”或者是一个“频危物种”的话题。因此，CNNs 成为了文本分类中一个很流行的模型架构之一。</p>
<p>Kalchbrenner 等人首次在文本分类领域提出了基于 CNN 的模型。这个模型使用动态的 k-max 池化，被称为动态 CNN（DCNN）。如下图所示，DCNN 的第一层为每个句子中的单词通过嵌入的方式构建了一个句子矩阵。然后是一个由动态 k-max 池化给定的具有动态池化层的交替宽卷积层的卷积结构，这个结构被用来生成句与句之间的特征映射，该映射能够显式地捕获单词和短语之间的长距离和短距离的关系。池化参数 k 可以根据句子的大小和卷积层次结构中的级别动态的选择。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224144113.png" alt="image-20201224144055678"></p>
<p>接着，Kim 提出了一种比 DCNN 更简单的基于 CNN 的文本分类模型，如下图所示。Kim 的模型只在顶层词向量上使用了一层卷积层，这些词向量从 一个无监督神经语言模型 word2vec 学习得到。Kim 也比较了四种不同的学习词嵌入的方法：（1）CNN-rand，所有词嵌入随机初始化，然后在训练的时候修正。（2）CNN-static，使用 word2vec 预训练模型，并且在训练的时候固定模型。（3）CNN-no-static，在每个任务训练的时候微调 word2vec 词嵌入模型。（4）CNN-multi-channel，使用两组词嵌入向量，都用 word2vec 初始化，一个在训练的时候更新，而另一个固定。这些基于 CNN 的模型提升了情感分析和问题分类的最新技术。</p>
<p>研究人员为提高基于 CNN 模型的性能做出了巨大的努力。Liu 等人，提出了一种新的基于 CNN 的模型，这个模型对 Kim-CNN 有两点修正。第一点，采用动态最大池化技术来捕获不同领域文档中的细粒度特征。第二点，在池化层和输出层之间加入一个隐藏的瓶颈层，该层来学习紧凑的表示，可以减小模型的尺寸提高模型的表现。在 48， 49 论文中，取代使用预训练低维词向量作为 CNNs 的输入，作者直接将 CNNs 应用到高维文本数据当中，用来学习小文本范围内的嵌入情况进行分类。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224144107.png" alt="image-20201224144106174"></p>
<p>字符级 CNNs 也被用于文本分类。Zhang 等人提出了第一个这样的模型。如下图所示，模型以固定大小的字符作为输入，使用 one-hot 进行编码，然后穿过带有池化操作六层卷积层和三层全连接层的深度卷积模型。Prusa 等人，提出了一种使用 CNNs 的文本编码方法，这种方法极大地减少内存消耗和训练时间。这种方法可以很好地适应字母表的大小，从而可以从原始文本中保留更多的信息，从而提高分类性能。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224145804.png" alt="image-20201224145803990"></p>
<p>有研究探讨词嵌入和 CNN 结构对模型性能的影响。受 VGG 和 ResNets 启发，Conneau 等人，提出了 Very Deep CNN（VDCNN）模型用于文本处理。这个模型直接在字符级别处理，并且只需要很小的卷积层和池化操作。这项研究表明，VDCNN 随着深度的增加性能有所提升。Duque 等，通过修正 VDCNN 的模型结构使之适应移动平台的限制，并保持了性能表现。他们仅损失了 0.4% 到 1.3% 的性能就把模型大小压缩了 10 到 20 倍。Le 等人，表明当文本输入被表示为一系列字符时，深层模型的性能表现确实好过浅层模型。然而，一个简单的浅而宽的网络 DenseNet 等具有单词输入的深层模型表现更加好。Guo 等人，研究了词嵌入的影响，并提出了使用通过多通道的 CNN 模型加权的词嵌入。Zhang 等人，测试了使用不同的词嵌入方法和池化机制，并且发现使用 non-static word2vec 和 Glove 词嵌入的性能要比使用 one-hot 向量的性能好，最大池化方法一直比其他池化方法好。</p>
<p>另一个有趣的基于 CNN 的模型。Mou 等人，提出了一种基于树形结构的 CNN 来捕获句子级别的语义。Pang 等人，将文本匹配任务转换为图像识别任务，并使用多层 CNNs 来验证 n-gram 模型。Wang 等人，提出了一种结合了显式和隐式表示的基于 CNN 的模型来进行短文本分类。将 CNNs 应用到生物文本分类领域也越来越火。</p>
<h3 id="Capsule-Neural-Networks"><a href="#Capsule-Neural-Networks" class="headerlink" title="Capsule Neural Networks"></a>Capsule Neural Networks</h3><p>CNNs 通过使用连续的卷积和池化层来进行图片和文本的分类。尽管池化操作可以识别显著的特征并降低卷积操作的计算复杂性，但它们会丢失有关空间关系的信息，并且很可能会根据实体的方向或比例对实体进行错误分类。</p>
<p>为了解决池化的问题，一种新的方法被 Geoffrey Hinton 提出，称为胶囊网络（CapsNets）。胶囊是一组神经元，其活动向量表示特定类型实体，例如对象或者对象的一部分。向量的长度表示实体存在的概率，向量的方向表示实体的属性。与 CNNs 的最大池化不同，CNNs 选择一些信息并丢弃另一些信息，胶囊网络将每个低维的胶囊“路由到”上层的最佳父胶囊，使用网络中所有的信息进行分类。路由可以通过不同的算法实现，比如协议动态路由或者 EM 算法。</p>
<p>最近，胶囊网络被应用到文本分类当中，胶囊被看做是一个向量，用来表示一个句子或者文档。 71-73 提出了一个基于 CapsNets 变种的文本分类模型。这个模型由四层组成：（1）一个 n-gram 卷积层，（2）一个胶囊层（3）一个卷积胶囊层，（4）和一个全连接胶囊层。作者尝试了三种策略来稳定动态路由过程，以减轻包含背景信息，如停用词或与任何文档类别无关的词的噪声胶囊的干扰。他们也探索了两种胶囊的结构，分别表示为 Capsule-A 和 Capsule-B，如下图所示。Capsule-A 和 CapsNet 相似。Capsule-B 在 n-gram 卷积层使用三个据欧不同窗口大小的过滤器的并行网络来学习更全面的文本表示。CapsNet-B 在实验中表现得更好。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224153952.png" alt="image-20201224153952607"></p>
<p>基于 CapsNet 的模型被 Kim 提出来，使用了一个相似的结构。这个模型由（1）一个将文档作为连续词嵌入的输入层，（2）一个生成特征映射并且使用线性门单元保留空间信息的卷积层，（3）一个通过卷积层整合检测到的局部特性形成全局特征的卷积胶囊层。走着观察到目标在文本中能够比在图像中更灵活的组合。举例说明，不像人类眼睛和鼻子在脸上的位置一样，文档的语义信息能够被保留，即使几个句子的顺序被调换。因此，他们使用静态路由策略，这种策略在文本分类中一直比动态路由表现更好。Aly 等人，提出了使用 CapsNets 进行多层多标签分类（HMC），他们认为 CapsNet 的编码父子之间的关系的能力使得它比传统的方法在 HMC 任务中表现得更好，这种传统的方式文档被以层次结构分为一个或多个类别标签。他们的模型结构和 71 72 74 相似。</p>
<p>Ren 等人提出另一种变种的 CapsNets，这种方式使用组合编码机制在胶囊和一个基于 k-means 聚类的新路由算法。首先，用编码本中的全部编码词向量生成词嵌入。然后由较低级别的胶囊捕获的特征通过 𝑘-means 路由聚合到高级胶囊中。</p>
<h3 id="Models-with-Attention-Mechanism"><a href="#Models-with-Attention-Mechanism" class="headerlink" title="Models with Attention Mechanism"></a>Models with Attention Mechanism</h3><p>注意力机制是由我们如何将视觉的注意力集中在不同区域或一个句子中的单词关联起来而激发的。在为 NLP 开发深度学习模型时，注意力成为一个越来越流行的概念和有用的工具。简而言之，语言模型中的注意力可以解释为权重向量。为了预测一个句子中的一个词，我们使用注意力向量来估计它与其他词的关联程度，或“注意到”其他词，并将它们的加权值之和作为目标的近似值。</p>
<p>这一节回顾了一些最突出的注意力模型，这些模型在文本分类任务上创造了新的研究现状。</p>
<p>Yang 等人提出了一种用于文本分类的层次注意力网络。该模型有两个显著的特点：（1）反映文档层次结构；（2）在词和句子两个层次上应用了注意力机制，使其在构建文档表示时能够区分关注更多和较不重要的内容。该模型在六个文本分类任务上的性能明显优于以往的方法。Zhou 等人将层次注意模型扩展到跨语言情感分类。在每种语言中，使用 LSTM 网络对文档进行建模。然后，采用层次注意机制进行分类，句子级注意模型学习文档中哪些句子对整体情感的决定更重要，而词级注意模型则学习每个句子中哪些词是决定性的。</p>
<p>Shen 等人提出了一个用于 RNN/CNN 自由语言理解的定向自我注意网络，其中来自输入序列的元素之间的注意具有方向性和多维性。利用一个轻量化的神经网络来学习句子嵌入，完全基于所提出的注意力机制，没有任何 RNN/CNN 结构。Liu 等人提出了一个具有内在关注的 NLI 的 LSTM 模型。该模型采用两阶段过程对句子进行编码。首先，在词级 Bi-LSTM 上使用平均池化来生成第一阶段的句子表示。其次，采用注意力机制来代替同一句子上的平均集中，以获得更好的表达效果。句子的第一阶段表征是用来处理出现在句子中的单词。</p>
<p>注意力模型也广泛应用于配对排序或匹配任务。Santos 等人提出了一种双向注意力机制，称为注意池（AP），用于成对排序。AP 使池化层能够知道当前的输入对（例如，问答对），这两个输入项的信息可以直接影响彼此表示的计算。AP 除了学习输入对的表示外，还联合学习对投影片段的相似性度量，然后针对每个输入导出相应的注意向量，以指导合并。AP 是独立于底层表示学习的一般框架，并且可以应用于 CNNs 和 RNNs，如图 a 所示。Wang 等将文本分类看作是一个标签词匹配问题：每个标签与词向量嵌入到同一空间。作者介绍了一个注意框架，该框架通过余弦相似性度量文本序列和标签之间嵌入的兼容性，如图 b 所示。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224161515.png" alt="image-20201224161515471"></p>
<p>Kim 等人提出了一种使用紧密连接的递归和协同注意力网络的语义句子匹配方法。与 DenseNet 相似，该模型的每一层都使用关注特征的串联信息以及所有前面递归层的隐藏特征。它能够从最底层的单词嵌入层到最上面的递归层保留原始和协同注意力的特征信息。Yin 等人提出了另一种基于注意力的 CNN 句子对匹配模型。他们研究了将句子间的相互影响整合到 CNN 中的三种注意力方案，以便每个句子的表示都考虑到它的成对句子。这些相互依存的句子对表征比孤立的句子表征更为有效，在包括答案选择、释义识别和文本蕴涵在内的多种分类任务中得到验证。Tan 等人在匹配聚合框架下利用多个注意力函数匹配句子对。Yang 等人介绍了一种基于注意力的神经匹配模型，用于对简短回答文本进行排序。他们采用价值共享加权方案代替位置共享加权方案来组合不同的匹配信号，并利用问题注意网络进行问题项重要性学习。该模型在 TREC QA 数据集上取得了良好的效果。</p>
<p>还有其他有趣的注意力模型。Lin 等人利用自注意力机制提取可解释的句子嵌入。Wang 等人提出了一种具有多尺度特征关注的稠密连接 CNN 来产生可变的 n-gram 特征。Yamada 和 Shindo 使用神经注意实体袋模型，使用知识库中的实体进行文本分类。Parikh 等人利用注意力将问题分解为可以单独解决的子问题。Chen 等人探索了广义池化方法来增强句子嵌入，并提出了一种基于向量的多头注意模型。Liu 和 Lane 提出了一种基于注意的 RNN 模型，用于联合意图检测和缝隙填充。</p>
<h3 id="Memory-Augmented-Networks"><a href="#Memory-Augmented-Networks" class="headerlink" title="Memory-Augmented Networks"></a>Memory-Augmented Networks</h3><p>当注意力模型在编码过程中存储的隐藏向量可以看作是模型内部记忆的条目时，记忆增强网络将神经网络与一种形式的外部记忆相结合，模型可以读取和写入外部记忆。<br>Munkhdalai 和 Yu 提出了一种记忆增强型神经网络，称为神经语义编码器（NSE），用于文本分类和问答。NSE配备了一个可变大小的编码存储器，该存储器随着时间的推移而演变，并且通过读和写操作来保持对输入序列的理解，如图所示。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224163135.png" alt="image-20201224163135734"></p>
<p>Weston 等人 为一个合成的 QA 任务设计一个记忆网络，在这个网络中，一系列的语句（内存条目）被提供给模型作为问题的支持事实。这个模型尝试去从基于记忆的问题和先前检索到的记忆中一次取出一个条目。Sukhbaatar 等人扩展了这项工作，并提出了端到端记忆网络，在这种网络中，通过注意力机制以软方式检索记忆条目，从而实现端到端的训练。他们表明，在多轮（hops）下，模型能够检索和推理几个支持事实来回答一个特定的问题。</p>
<p>Kumar 等人提出了一种动态记忆方法（DMN），它处理输入序列和问题，形成情景记忆，并生成相关答案。问题触发了一个迭代的注意过程，这个过程允许模型根据输入和先前迭代的结果来调整它的注意。然后，这些结果在递阶递归序列模型中进行推理，以生成答案。DMN 进行了端到端的训练，并获得了 QA 和 POS 标记方面的最新结果。Xiong 等人对 DMN 进行了详细的分析，并对其记忆和输入模块进行了改进。</p>
<h3 id="Graph-Neural-Networks"><a href="#Graph-Neural-Networks" class="headerlink" title="Graph Neural Networks"></a>Graph Neural Networks</h3><p>自然语言文本虽然具有顺序性，但也包含了内部的图结构，如句法和语义解析树，它们定义了句子中单词之间的句法/语义关系。</p>
<p>TextRank 是为 NLP 开发的最早的基于图形的模型之一。作者提出将自然语言文本表示为一个图𝐺（𝑉，𝐸），其中 𝑉 表示一组节点，𝐸 表示节点之间的一组边。根据手头的应用程序，节点可以表示各种类型的文本单元，例如单词、词的搭配、整个句子等。同样，可以使用边来表示任何节点之间的不同类型的关系，例如词汇或语义关系、上下文重叠等。</p>
<p>现代图神经网络（GNNs）是通过扩展图数据的深度学习方法发展起来的，如 TextRank 使用的文本图。深层神经网络，如 CNNs，RNNs 和自动编码器，在过去的几年里已经被广泛应用于处理复杂的图形数据。例如，用于图像处理的 CNNs 的二维卷积被推广到通过取节点邻域信息的加权平均来执行图卷积。在各种类型的神经网络中，卷积神经网络是最受欢迎的一类，如图卷积网络及其变种，因为它们能够有效、方便地与其他神经网络组合，并在许多应用中取得了最新的成果。GCNs 是图上 CNNs 的一个有效变体。GCNs 将学习到的一阶谱滤波器层叠起来，然后用非线性激活函数学习图形表示。<br>GNNs 在自然语言处理中的一个典型应用是文本分类。GNNs 利用文档或单词之间的相互关系来推断文档标签。接下来，我们将回顾一些为文本分类而开发的 GCNs 变体。</p>
<p>Peng 等人提出了一种基于图 CNN 的深度学习模型，首先将文本转换为文字图，然后使用图卷积运算对文字图进行卷积，如图所示。他们通过实验表明，文本的词图表示具有捕捉非连续、远距离语义的优势，CNN 模型具有学习不同层次语义的优势。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224164544.png" alt="image-20201224164544807"></p>
<p>在 106 中，Peng 等人提出了一种基于层次分类意识和注意力图胶囊的文本分类模型。该模型的一个独特之处是它使用了类标签之间的层次关系，在以前的方法中，这些关系被认为是独立的。具体地说，为了利用这些关系，作者开发了一种层次分类嵌入方法来学习它们的表示，并通过结合标签表示的相似性来定义一种新的加权边际损失。</p>
<p>Yao 等人使用类似的图 CNN（GCNN）模型进行文本分类。他们基于单词共现和文档单词关系为语料库构建一个文本图，然后学习该语料库的文本图卷积网络（Text GCN），如图所示。文本 GCN 初始化为单词和文档的一个 one-hot 表示，然后在已知的文档类标签的监督下，共同学习单词和文档的嵌入。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224165140.png" alt="image-20201224165140598"></p>
<p>构建大规模文本语料库的 GNNs 代价高昂。通过降低模型复杂度或改变模型训练策略来降低建模成本已成为研究热点。前者的一个例子是简单图卷积（SGC）模型，其中通过重复地去除连续层之间的非线性并将得到的函数（权重矩阵）折叠成单个线性变换来简化深度卷积 GNN。后者的一个例子是文本级 GNN。文本级 GNN 不是为一个完整的文本语料库建立一个图形，而是为文本语料库上的滑动窗口定义的每个文本块生成一个图形，以减少训练过程中的内存消耗。其他一些有前途的基于 GNN 的著作包括 GraphSage 和上下文化的非局部神经网络。</p>
<h3 id="Siamese-Neural-Networks"><a href="#Siamese-Neural-Networks" class="headerlink" title="Siamese Neural Networks"></a>Siamese Neural Networks</h3><p>孪生神经网络（S2Nets）及其 DNN 变体，即深结构语义模型（DSSM），是为文本匹配而设计的。这项任务是许多 NLP 应用的基石，例如提取式 QA 系统中的问题文档排序和回答选择。这些任务可以看作是文本分类的特殊情况。例如，在有问题的文档排序中，我们希望将文档分类为与给定查询相关或无关。</p>
<p>如图所示，DSSM（或 S2Net ）由一对 DNNs、𝑓1 和 𝑓2 组成，它们将输入 𝑥 和 𝑦 映射到公共低维语义空间中的相应向量中。然后用两个向量的余弦距离来衡量 𝑥 和 𝑦 的相似性。S2Nets 假定 𝑓1 和 𝑓2 共享相同的体系结构甚至相同的参数，而在 DSSMs 中，𝑓1 和 𝑓2 可以是不同的体系结构，具体取决于 𝑥 和 𝑦。例如，为了计算图像和文本对的相似性，𝑓1 可以是深层 CNN，而 𝑓2 可以是 RNN 或 MLP。<br>根据（𝑥，𝑦）的定义，这些模型可应用于各种 NLP 任务。例如，（𝑥，𝑦）可以是查询文档排名的查询文档对，也可以是 QA 中的问答对，依此类推。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224170002.png" alt="image-20201224170002587"></p>
<p>模型参数 𝜃 通常使用成对秩损失进行优化。以文档排名为例。考虑一个查询 𝑥 和两个候选文档 $y^+$ 和 $y^-$，其中 𝑦+ 与 𝑥 相关，而 𝑦- 不相关。设 sim𝜃（𝑥，𝑦）是由 𝜃 参数化的语义空间中 𝑥 和 𝑦 的余弦相似性。训练目标是将基于利润的损失最小化</p>
<script type="math/tex; mode=display">
L(\theta)=[\gamma + sim_{\theta}(x,y^-) - sim_{\theta}(x,y^+)]_+</script><p>其中，[𝑥]+:=max（0，𝑥），且 𝛾 是边距超参数。</p>
<p>由于文本呈现出一个有序的顺序，因此很自然地使用 RNNs 或 LSTMs 来度量文本之间的语义相似性。如图显示了 Mueller 等人提出的孪生模型的结构，其中两个网络使用相同的 LSTM 模型。Neculou 等人提出了一个类似的模型，该模型使用字符级 Bi-LSTM 来计算 𝑓1 和 𝑓2，并使用余弦函数来计算相似度。Liu 等人用两个耦合的 LSTMs 来模拟一个句子对的交互作用。除了 RNNs 外，在 S2Nets 中还使用了 BOW 模型和 CNNs 来表示句子。例如，He 等提出了一个 S2Net，它使用 CNNs 对多视角句子相似度进行建模。Renter 等人提出了一种连体 CBOW 模型，该模型通过对句子中的单词嵌入量进行平均来形成句子向量表示，并将句子相似度计算为句子向量之间的余弦相似度。随着 BERT 成为新的最先进的句子嵌入模型，人们试图构建基于 BERT 的 S2Nets，如 SBERT 和 TwinBERT。</p>
<p>S2Nets 和 DSSMs 已被广泛应用于 QA。Das 等人提出了一个孪生 CNN for Question-Answer（SCQA）来衡量问题与其（候选）答案之间的语义相似性。为了降低计算复杂度，SCQA 使用了问答对的字符级表示。SCQA 的参数被训练成最大化问题与其相关答案之间的语义相似性，如等式1，其中𝑥是一个问题，𝑦是它的候选答案。Tan 等人提出了一系列用于答案选择的孪生神经网络。如图所示，这些是使用卷积、递归和注意神经网络处理文本的混合模型。为 QA 开发的其他连体神经网络包括基于非因素答案选择的 LSTM 模型，双曲表示学习，以及使用深度相似神经网络的问题回答。</p>
<h3 id="Hybrid-Model"><a href="#Hybrid-Model" class="headerlink" title="Hybrid Model"></a>Hybrid Model</h3><p>许多混合模型已经被开发来结合 LSTM 和 CNN 架构来捕获句子和文档的局部和全局特征。Zhu 等人提出了一种卷积 LSTM（C-LSTM）网络。如图 a 所示，C-LSTM 利用 CNN 来提取高级短语（n-gram）表示序列，这些表示被馈送到 LSTM 网络以获得句子表示。同样，Zhang 等人提出了一个依赖敏感的 CNN（DSCNN）来进行文档建模。如图 b 所示，DSCNN 是一个分层模型，其中 LSTM 学习输入到卷积和最大池化层的句子向量，以生成文档表示。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201224171822.png" alt="image-20201224171822411"></p>
<p>Chen 等人通过 CNN-RNN 模型进行多标签文本分类，CNN-RNN 模型能够捕获全局和局部文本语义，因此能够建模高阶标签相关性，同时具有可处理的计算复杂性。Tang 等人使用 CNN 学习句子表征，使用门控 RNN 学习编码句子之间内在关系的文档表征。Xiao 等人将文档视为字符序列，而不是单词，并建议使用基于字符的卷积和递归层进行文档编码。与单词级模型相比，该模型在参数较少的情况下取得了可比的性能。递归的 CNN 应用递归结构来捕捉长范围的上下文依赖性来学习单词表征。为了降低噪声，采用最大池化法自动选择对文本分类任务至关重要的显著词。</p>
<p>Chen 等人受不同类型的句子表达情感方式不同的启发，提出了一种分治的方法，通过句子类型分类进行情感分析。作者首先应用 Bi-LSTM 模型将句子分为三类。然后将每组句子分别送入一维 CNN 进行情感分类。</p>
<p>在[135]中，Kowsari 等人提出了一种用于文本分类的分层深度学习方法（HDLTex）。HDLTex 采用混合的深度学习模型体系结构，包括 MLP、RNN 和 CNN，以在文档层次结构的每个级别提供专门的理解。</p>
<p>Liu 提出了一种鲁棒的随机答案网络（SAN），用于机器阅读理解中的多步推理。SAN 结合了不同类型的神经网络，包括记忆网络、Transforms、Bi-LSTM、注意力机制和 CNN。Bi-LSTM 组件获取问题和段落的上下文表示。它的注意力机制衍生出一种问题意识的语篇表征。然后，另一个 LSTM 被用来为该通道生成一个工作存储器。最后，基于选通递归单元（GRU）的应答模块输出预测。</p>
<p>一些研究集中于将 highway networks 与 RNN 和 CNN 相结合。在典型的多层神经网络中，信息是逐层流动的。随着深度的增加，基于梯度的 DNN 训练变得越来越困难。Highway networks 的设计是为了简化对非常深的神经网络的训练。它们允许信息在信息高速公路上的多个层之间畅通无阻地流动，类似于 ResNet 中的快捷连接。Kim 等人利用 CNN 和 LSTM 在字符上的高速公路网络进行语言建模。如图 16 所示，第一层执行字符嵌入的查找，然后应用卷积和最大池化运算来获得字的固定维表示，该字由 highway network 给出。Highway network 的输出被用作多层 LSTM 的输入。最后，一个仿射变换和一个 softmax 被应用于 LSTM 的隐藏表示，以获得下一个单词的分布。其他基于公路的混合模型包括 Recurrent Highway Networks 和 RNN with highway。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228144030.png" alt="image-20201228144021899"></p>
<h3 id="Transformers-and-Pre-Trained-Language-Models"><a href="#Transformers-and-Pre-Trained-Language-Models" class="headerlink" title="Transformers and Pre-Trained Language Models"></a>Transformers and Pre-Trained Language Models</h3><p>RNNs 的计算瓶颈之一是文本的顺序处理。虽然 CNN 比 RNN 的顺序性要小，但是捕捉句子中单词之间关系的计算开销也会随着句子长度的增加而增加，这与 RNNs 类似。Transformers 克服了这一局限性，通过应用 self-attention 机制来并行计算句子中的每个单词或记录的“注意力分数”来对每个词和另一个词的影响建模。因为这种特性，使得 Transformers 比 CNNs 和 RNNs 有更好并行化，这使得使用在 GPU 集群上高效地训练建立在大型数据集上的大模型变为可能。</p>
<p>自 2018 年以来，我们看到了一系列基于 Transformers 的大规模 Pre-Trained Language Models 语言模型（PLM）的兴起。与早期基于 CNNs 或 LSTMs 的上下文化嵌入模型相比，基于 Transformer 的 PLMs 使用更深层的网络架构（例如 48 层 Transformers），并在大量文本语料库上预先训练，通过预测上下文中的单词来学习上下文文本表示。这些 PLMs 已经使用特定于任务的标签进行了 fine-tuned，并在许多下游 NLP 任务（包括文本分类）中取得了行的成绩。虽然预训练是无监督（或自监督）的，但 fine-tuning 是有监督的学习。</p>
<p>PLMs 可分为两类，自回归和自动编码 PLMs。最早的自回归模型之一是 OpenGPT，它是一种单向模型，它从左到右（或从右到左）逐字预测文本序列，每个单词的预测都取决于先前的预测。图 17 显示了 OpenGPT 的体系结构。它由 12 层 Transformer 块组成，每层由一个屏蔽的多头 Attention 模块组成，然后是一个层标准化和一个位置前馈层。OpenGPT 可以通过添加特定于任务的线性分类器和使用特定于任务的标签进行 fine-tuning 来适应文本分类等下游任务。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228145952.png" alt="image-20201228145951781"></p>
<p>BERT 是自编码 PLMs 应用最广泛的模型之一。和 OpenGPT 基于先前预测预测单词，使用遮盖的语言建模任务训练 BERT，该任务在文本序列中随机掩蔽一些标记，然后通过对双向 Transformer 获得的编码向量进行调节来独立地恢复掩蔽的标记。有很多关于改善 BERT 的工作。RoBERTa 比 BERT 更健壮，并且使用更多的训练数据进行训练。ALBERT 降低了内存消耗，提高了 BERT 的训练速度。DistillBERT 在预训练期间利用知识蒸馏将 BERT 的大小减少 40%，同时保留 99% 的原始性能，并且使得推理速度快 60%。SpanBERT 扩展了 BERT 来更好地表示和预测文本跨度。BERT 及其变体已经针对各种 NLP 任务进行了 fine-tuned，包括QA、文本分类和 NLI。</p>
<p>有人试图结合自回归和自动编码 PLMs 的优势。XLNet 集成了自回归模型的思想，如 OpenGPT 和 BERT 的双向上下文建模。XLNet 在预训练期间使用了一个置换操作，允许上下文包含来自左和右的标记，使其成为一个通用的顺序感知自回归语言模型。这种排列是通过在 Tansformers 中使用一个特殊的注意遮蔽来实现的。XLNet 还引入了一个双流自注意力模式，允许位置感知单词预测。这是由于观察到单词分布因单词位置的不同而有很大差异。例如，一个句子的开头与句子中的其他位置有很大的不同。如图 18 所示，为了在排列 3-2-4-1 中预测位置 1 的单词标记，通过包括所有先前单词（3、2、4）的位置嵌入和标记嵌入来形成内容流，然后通过包含要预测的单词（位置 1 中的单词）的内容流和位置嵌入来形成查询流，最后根据查询流中的信息进行预测。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228151351.png" alt="image-20201228151350709"></p>
<p>如前所述，OpenGPT 使用从左到右的 Transformer 来学习文本表示以生成自然语言，而 BERT 使用双向 Transformer 来理解自然语言。统一语言模型（UniLM）旨在处理自然语言理解和生成任务。UniLM 使用三种类型的语言建模任务进行预训练：单向、双向和序列到序列预测。如图 19 所示，通过使用共享的 Transformer 网络和使用特定的自我注意力掩码来控制预测条件所处的上下文来实现统一建模。据报道，UniLM 的第二个版本在广泛的自然语言理解和生成任务上达到了最新的水平，显著优于以前的 PLMs，包括 OpenGPT-2、XLNet、BERT及其变体。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228151805.png" alt="image-20201228151804228"></p>
<p>Raffel 等人提出了一个统一的基于 Transformer 的框架，可以将许多 NLP 问题转换为文本到文本的格式。他们还进行了一项系统的研究，以比较训练前的目标、体系结构、未标记的数据集、微调方法和其他因素对几十项语言理解任务的影响。</p>
<h3 id="Beyond-Supervised-Learning"><a href="#Beyond-Supervised-Learning" class="headerlink" title="Beyond Supervised Learning"></a>Beyond Supervised Learning</h3><p><strong>Unsupervised Learning using Autoencoders</strong></p>
<p>与单词嵌入类似，句子的分布式表示也可以在无监督的方式下学习。通过优化一些辅助目标，例如自动编码器的重建损失。这种无监督学习的结果是句子编码器，它可以将具有相似语义和句法特性的句子映射到相似的固定大小的向量表示。<br>第 2.10 节中描述的基于 Transformer 的 PLMs 也是无监督模型，可以用作句子编码器。本节讨论基于自动编码器及其变体的无监督模型。</p>
<p>Kiros 等人提出了一种跳跃思维模型，用于非监督学习通用的句子编码器。通过训练编译码器模型来重构编码句子的周围句子。Dai 和 Le 研究序列自动编码器的使用，它将输入序列读入向量并再次预测输入，用于句子编码。他们表明，在一个大的无监督语料库上进行预训练的句子编码器比只进行预训练的单词嵌入更准确。Zhang 等人提出了一种平均最大注意力自动编码器，它利用多头自注意力机制重构输入序列。在编码中使用了 mean-max 策略，其中对隐藏向量的 mean 和 max 池化操作都被应用于捕获输入的不同信息。</p>
<p>当自动编码器学习输入的压缩表示时，变分自动编码器（VAEs）学习表示数据的分布，可以看作是自动编码器的正则化版本。因为 VAE 学会了对数据建模，所以我们可以很容易地从分布中抽取样本来生成新的输入数据样本（例如，新的句子）。Miao 等人将 VAE 框架扩展到文本，提出了用于文档建模的神经变分文档模型（NVDM）和用于 QA 的神经答案选择模型（NASM）。如图20（a）所示，NVDM 使用 MLP 编码器将文档映射到连续语义表示。如图 20（b）所示，NASM 使用 LSTM 和潜在随机注意机制来建模问答对的语义，并预测它们的相关性。注意模型关注与问题语义紧密相关的答案短语，并通过潜在分布建模，允许模型处理任务中固有的歧义。Bowman等人提出了一个基于 RNN 的 VAE 语言模型，如图20（c）所示。该模型包含了整个句子的分布式潜在表示，允许显式地模拟句子的整体属性，如风格、主题和高级句法特征。Gururangan 等人预先训练文档模型作为域内未标记数据的 VAE，并使用其内部状态作为文本分类的特征。一般来说，使用 VAE 或其他模型的数据扩充被广泛应用于半监督或弱监督文本分类。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228152545.png" alt="image-20201228152545004"></p>
<p><strong>Adversarial Training</strong></p>
<p>对抗训练是一种改进分类器泛化的正则化方法。它通过提高模型对对抗性示例的鲁棒性来做到这一点，对抗性示例是通过对输入进行小扰动而创建的。对抗性训练需要使用标签，并应用于监督学习。虚拟对抗训练将对抗训练扩展到半监督学习。这是通过正则化一个模型来实现的，这样，给定一个例子，该模型产生的输出分布与该例子的对抗性扰动产生的输出分布相同。Miyato 等人扩展对抗和虚拟对抗训练，通过对 RNN 中的单词嵌入而不是原始输入本身施加扰动来完成监督和半监督的文本分类任务。Sachel 等人研究半监督文本分类的 LSTM 模型。他们发现，使用混合目标函数，将交叉熵、对抗性和虚拟对抗性损失结合起来，对有标记和未标记的数据，可以显著改善监督学习方法。Liu 等将对抗训练扩展到文本分类的多任务学习框架，旨在减轻任务独立（共享）和任务相关（私有）的潜在特征空间相互干扰。</p>
<p><strong>Reinforcement Learning</strong></p>
<p>强化学习（Reinforcement learning，RL）是一种训练代理根据策略执行离散动作的方法，该策略被训练为使奖励最大化。Shen 等人使用硬注意模型来选择输入序列中关键词标记的子集进行文本分类。硬注意模型可以看作是一个代理，它执行是否选择令牌的操作。在遍历整个文本序列之后，它会收到一个分类损失，作为训练代理的奖励。Liu 等人提出了一种神经代理，它将文本分类建模为一个序列决策过程。受人类文本阅读认知过程的启发，代理按顺序扫描一段文本，并在其希望的时间做出分类决策。分类结果和何时进行分类都是决策过程的一部分，由 RL 训练的策略控制。Shen 等提出了一种机器阅读理解的多步推理网络（ReasoNet）。ReasoNets 任务多个步骤来分析查询、文档和答案之间的关系。在推理过程中，ReasoNets 没有使用固定数量的步骤，而是引入终止状态来放松对推理步骤的约束。使用 RL，推理机可以动态地决定是否在消化中间结果后继续理解过程，或者在得出现有信息足以产生答案时终止阅读。Li 等人将 RL、GANs 和 RNNs 结合起来，建立了一个新的模型，称为类别句生成对抗网络（CS-GAN），该模型能够在监督训练过程中生成扩大原始数据集的类别句，并提高其泛化能力。Zhang 等人提出了一种基于 RL 的文本分类结构化表示学习方法。他们提出了两个基于 LSTM 的模型。第一种方法只在输入文本中选择与任务相关的重要单词。另一个发现句子的短语结构。使用这两个模型的结构发现被描述为由策略网络指导的顺序决策过程，策略网络在每个步骤决定使用哪个模型，如图 21 所示。利用策略梯度优化策略网络。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228153049.png" alt="image-20201228153048414"></p>
<p>图 22 显示了自 2013 年以来流行的基于 DL 的文本分类和嵌入工作的时间轴。鉴于在过去几年中发展了大量的作品，我们只展示了一些最具代表性的作品。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228153134.png" alt="image-20201228153133192"></p>
<h2 id="How-To-Choose-The-Best-Neural-Network-Model-For-My-Task"><a href="#How-To-Choose-The-Best-Neural-Network-Model-For-My-Task" class="headerlink" title="How To Choose The Best Neural Network Model For My Task"></a>How To Choose The Best Neural Network Model For My Task</h2><p>“什么是文本分类的最佳神经网络架构”的答案？“根据目标任务和域的性质、域内标签的可用性、应用程序的延迟和容量限制等，差异很大。尽管毫无疑问，开发文本分类器是一个反复试验的过程，但通过分析公共基准测试（如GLUE）的最新结果，我们提出了以下方法来简化该过程,包括五个步骤：</p>
<ol>
<li><strong>PLM Selection</strong>。如第 5 节所示，使用预先训练的语言模型（PLM）可以显著改善所有流行的文本分类任务，而自动编码 PLM（如 BERT 或 RoBERTa）通常比自回归 PLM（例如 OpenAI GPT）更有效。Hugging Face 维护了为各种任务和设置开发的丰富的 PLM 库。</li>
<li><strong>Domain adaptation</strong>。大多数 PLMs 都是在通用领域文本语料库（如 Web）上培训的。如果目标领域与一般领域有很大的不同，我们可以考虑通过对选定的通用领域 PLM 进行连续的预训练来适应使用领域内数据的PLM。对于有大量未标记文本的领域，如生物医学，从头开始预先训练语言模型也是一个不错的选择。</li>
<li><strong>Task-specific model design</strong>。给定一个输入文本，PLM 在上下文表示中生成一系列向量。然后，在顶部添加一个或多个特定于任务的层，以生成目标任务的最终输出。任务特定层结构的选择取决于任务的性质，例如，需要捕获文本的语言结构。如第 2 节所述，前馈神经网络将文本视为一个词袋，RNN 可以捕获词序，CNN 擅长识别关键短语等模式，注意力机制可以有效识别文本中的相关词，双生神经网络用于文本匹配任务，如果自然语言的图结构（如解析树）对目标任务有用，那么 GNNs 是一个很好的选择。</li>
<li><strong>Task-specific fine-tuning</strong>。根据领域内标签的可用性，任务特定层可以单独使用 PLM 进行训练，也可以与PLM 一起训练。如果需要构建多个相似的文本分类器（例如，针对不同领域的新闻分类器），多任务 fine-tuning 是利用相似领域的标记数据的一个不错的选择。</li>
<li><strong>Model compression</strong>。PLMs 的服务成本很高。它们通常需要通过知识蒸馏（knowledge distillation）进行压缩，以满足实际应用中的延迟和容量限制。</li>
</ol>
<h1 id="Text-Classification-Datasets"><a href="#Text-Classification-Datasets" class="headerlink" title="Text Classification Datasets"></a>Text Classification Datasets</h1><p>在本节中，我们将描述一些广泛用于文本分类研究的数据集。我们根据这些数据集的主要目标应用，将其分为情绪分析、新闻分类、主题分类、问答和 NLI 等类别。</p>
<h3 id="Sentiment-Analysis-Datasets"><a href="#Sentiment-Analysis-Datasets" class="headerlink" title="Sentiment Analysis Datasets"></a>Sentiment Analysis Datasets</h3><p><strong>Yelp</strong></p>
<p>Yelp 数据集包含两个情绪分类任务的数据。一种是检测细粒度的情绪标签，被称为 Yelp-5。另一种预测消极和积极的情绪，被称为 Yelp Review Polarity 或 Yelp-2。Yelp-5 有 65 万个训练样本，每个类有 5 万个测试样本；Yelp-2 包括 56 万个训练样本和 3.8 万个消极和积极类别的测试样本。</p>
<p><strong>IMDb</strong></p>
<p>IMDB 数据集是为电影评论的二元情感分类而开发的。IMDB 由相同数量的正面和负面评论组成。它平均分为训练集和测试集，每个测试集有 25000 条评论。</p>
<p><strong>Movie Review</strong></p>
<p>电影评论（MR）数据集是电影评论的集合，其任务是检测与特定评论相关的情绪并确定其是负面还是积极的。它包括 10662 个句子和偶数的正负样本。10 倍交叉验证与随机分裂通常用于测试这个数据集。</p>
<p><strong>SST</strong></p>
<p>斯坦福情感树库（SST）数据集是 MR 的扩展版本。有两个版本可用，一个带有细粒度标签（五类），另一个是二进制标签，分别称为 SST-1 和 SST2。SST-1 包括 11855 个电影评论，分为 8544 个培训样本、1101 个开发样本和 2210 个测试样本。SST-2 分为三组，大小分别为 6920、872 和 1821，分别作为训练集、开发集和测试集。</p>
<p><strong>MPQA</strong></p>
<p>多视角问答（MPQA）数据集是一个带有两个类标签的意见语料库。MPQA 由 10606 个句子组成，这些句子是从与各种各样的新闻来源相关的新闻文章中提取出来的。这是一个不平衡的数据集，有 3311 个正文档和 7293 个负文档。</p>
<p><strong>Amazon</strong></p>
<p>这是从亚马逊网站收集的一个流行的产品评论语料库。它包含二进制分类和多类（5类）分类的标签。Amazon 二进制分类数据集分别包含 3600000 条和 400000 条用于培训和测试的评论。Amazon 5-class 分类数据集（Amazon-5）分别由 300 万条和 65 万条培训和测试评论组成。</p>
<h3 id="News-Classification-Datasets"><a href="#News-Classification-Datasets" class="headerlink" title="News Classification Datasets"></a>News Classification Datasets</h3><p><strong>AG News</strong></p>
<p>AG 新闻数据集是学术新闻搜索引擎 ComeToMyHead 从 2000 多个新闻来源收集的新闻文章的集合。这个数据集包括 120000 个训练样本和 7600 个测试样本。每个示例都是带有四个类标签的短文本。</p>
<p><strong>20 Newsgroups</strong></p>
<p>20 个新闻组数据集是发布在 20 个不同主题上的新闻组文档的集合。该数据集的各种版本用于文本分类、文本聚类等。其中一个最流行的版本包含 18821 个文档，这些文档在所有主题中平均分类。</p>
<p><strong>Sougou News</strong></p>
<p>搜狗新闻数据集是 SogouCA 和 SogouCS 新闻语料库的混合体。新闻的分类标签由其在 URL 中的域名决定。例如，带有 URL 的新闻<a href="http://sports.sohu.com是归类为运动类。">http://sports.sohu.com是归类为运动类。</a></p>
<p><strong>Reuters news</strong></p>
<p>《路透社》杂志（Reuters）在 1987 年的《金融分类》中广泛使用的分类数据来自《路透社》杂志《5186》杂志，其中最常用的分类数据是1987年《路透社》杂志收集的。ApteMod 是路透社 21578 的多类版本，包含10788 个文档。它有 90 个班级，7769 份培训文件和 3019 份测试文件。来自路透社数据集子集的其他数据集包括 R8、R52、RCV1 和 RCV1-v2。<br>为新闻分类开发的其他数据集包括：必应新闻、英国广播公司、谷歌新闻。</p>
<h3 id="Topic-Classification-Datasets"><a href="#Topic-Classification-Datasets" class="headerlink" title="Topic Classification Datasets"></a>Topic Classification Datasets</h3><p><strong>DBpedia</strong></p>
<p>DBpedia 数据集是一个大规模的多语言知识库，它是由 Wikipedia 中最常用的信息框创建的。DBpedia 每月发布一次，并且在每个版本中添加或删除一些类和属性。最流行的 DBpedia 版本包含 560000 个训练样本和 70000 个测试样本，每个样本都有 14 个类标签。</p>
<p><strong>Ohsumed</strong></p>
<p>Ohsumed 集合是 MEDLINE 数据库的一个子集。Ohsumed 包含 7400 份文件。每个文档都是一个医学摘要，由从 23 个心血管疾病类别中选择的一个或多个类别进行标记。</p>
<p><strong>EUR-Lex</strong></p>
<p>EUR-Lex 数据集包括不同类型的文档，这些文档根据多个正交分类方案进行索引，以允许多个搜索工具。该数据集最受欢迎的版本基于欧盟法律的不同方面，共有 19314 份文件和 3956 个类别。</p>
<p><strong>WOS</strong></p>
<p>科学网（WOS）数据集是科学网的数据和元数据的集合，科学网是世界上最值得信赖的独立于出版商的全球引文数据库。WOS 发布了三个版本：WOS-46985、WOS-11967 和 WOS-5736。WOS-46985 是完整的数据集。WOS-11967 和 WOS-5736 是 WOS-46985 的两个子集。</p>
<p><strong>PubMed</strong></p>
<p>PubMed 是美国国家医学图书馆为医学和生物科学论文开发的搜索引擎，其中包含一个文档集合。每个文档都用网格集的类来标记，网格集是 PubMed 中使用的一个标签集。摘要中的每个句子都用下列类别之一标记其在摘要中的角色：背景、目标、方法、结果或结论。</p>
<p>其他用于主题分类的数据集包括 PubMed 200k RCT，Irony（由来自社交新闻网站 reddit 的注释组成，Twitter 用于推特主题分类的数据集，arXiv collection），仅举几例。</p>
<h3 id="QA-Datasets"><a href="#QA-Datasets" class="headerlink" title="QA Datasets"></a>QA Datasets</h3><p><strong>SQuAD</strong></p>
<p>斯坦福问答数据集（SQuAD）是一组来自维基百科文章的问答对。在 SQuAD，问题的正确答案可以是给定课文中的任何一系列标记。因为问题和答案是由人类通过众包产生的，它比其他一些问答数据集更加多样化。SQuAD 1.1包含 107785 对 536 篇文章的问答。<br>SQuAD2.0 是最新版本，它将 SQuAD1.1 中的 100000 个问题与超过 50000 个由众包商以类似于可回答问题的形式恶意编写的不可回答的问题相结合。</p>
<p><strong>MS MARCO</strong></p>
<p>此数据集由 Microsoft 发布。与 SQuAD 不同的是，所有问题都是通过编辑产生的；在 MS MARCO 上，所有问题都是通过 Bing 搜索引擎从用户的查询和真实的网络文档中抽取的。MS MARCO 的一些答案是有创造性的。因此，数据集可以用来开发生成性的质量保证系统。</p>
<p><strong>TREC-QA</strong></p>
<p>TREC-QA 是 QA 研究中最受欢迎和研究的数据集之一。这个数据集有两个版本，称为 TREC-6 和 TREC-50。在 TREC-6 中包含 6 个分类，TREC-50 包含 50 个分类。对于这两个版本，训练和测试数据集分别包含 5452 和 500 个问题。</p>
<p><strong>WikiQA</strong> </p>
<p>WikiQA  数据集由一组问答对组成，为开放域 QA 研究收集和注释。数据集还包括没有正确答案的问题，允许研究人员评估答案触发模型。 </p>
<p><strong>Quora</strong></p>
<p>Quora 数据集是为释义识别（检测重复问题）而开发的。为此，作者展示了 Quora 数据的一个子集，它包含了超过 400000 个问题对。一个二进制值被分配给每个问题对，指示这两个问题是否相同。<br>QA 的其他数据集包括SWAG，WikiQA，SelQA。</p>
<h3 id="NLI-Datasets"><a href="#NLI-Datasets" class="headerlink" title="NLI Datasets"></a>NLI Datasets</h3><p><strong>SNLI</strong></p>
<p>斯坦福自然语言推理（SNLI）数据集被广泛用于自然语言推理。该数据集由 550152、10000 和 10000 个句子对组成，分别用于训练、发展和测试。每一对都用三个标签中的一个来标注：中性、蕴涵、矛盾。</p>
<p><strong>Multi-NLI</strong></p>
<p>多体裁自然语言推理（MNLI）数据集是 433k 个句子对的集合，这些句子对带有文本蕴涵标签。语料库是 SNLI 的延伸，涵盖了更广泛的口语和书面语篇体裁，支持独特的跨体裁泛化评价。</p>
<p><strong>SICX</strong></p>
<p>包含复合知识（SICK）数据集的句子由大约 10000 个英语句子对组成，这些句子被标注为三个标签：蕴涵、矛盾和中性。</p>
<p><strong>MSRP</strong></p>
<p>微软研究释义（MSRP）数据集通常用于文本相似性任务。MSRP 包括 4076 个培训样本和 1725 个测试样本。每个示例是一个句子对，用一个二进制标签标注，指示这两个句子是否是意译。<br>其他 NLI 数据集包括语义文本相似性 STS，RTE，SciTail，等等。</p>
<h2 id="Experimental-Performance-Analysis"><a href="#Experimental-Performance-Analysis" class="headerlink" title="Experimental Performance Analysis"></a>Experimental Performance Analysis</h2><p>在这一部分中，我们首先描述了一组常用于评估文本分类模型性能的指标，然后对一组基于深度学习的文本分类模型的性能进行了定量分析。</p>
<h3 id="Popular-Metrics-for-Text-Classification"><a href="#Popular-Metrics-for-Text-Classification" class="headerlink" title="Popular Metrics for Text Classification"></a>Popular Metrics for Text Classification</h3><p><strong>Accuracy and Error Rate</strong></p>
<p>这些是评估分类模型质量的主要指标。设 TP、FP、TN、FN 分别表示真阳性、假阳性、真阴性和假阴性。分类精度和错误率在公式 2 中定义。</p>
<script type="math/tex; mode=display">
Accuracy = \frac{TP+TN}{N} \\
Error \ rate  = \frac{FP+FN}{N}</script><p>N 是样本总数，显而易见的，有 $Error rate = 1 - Accuracy$</p>
<p><strong>Precision/Recall/F1 score</strong></p>
<p>这些也是主要的度量标准，对于不平衡的测试集，它们比准确度或错误率更常用，例如，大多数测试样本都有一个类标签。二元分类的精确度和召回率定义为等式 3。F1 分数是准确度和召回率的调和平均值，如等式 3 所示。F1 分数在 1 时达到最佳值（精确性和召回率），最差值为 0。</p>
<script type="math/tex; mode=display">
Precision = \frac{TP}{TP+FP} \\
Recall = \frac{TP}{TP+FN} \\
F1-score = \frac{2 \ Prec \ Rec}{Prec + Rec}</script><p>对于多类分类问题，我们可以计算每个类标签的精确性和召回率，并分析每个类标签的性能，或者平均这些值，从而得到整体的准确度和召回率。</p>
<p><strong>Exact Match(EM)</strong></p>
<p>精确匹配度量是问答系统中一个流行的度量，它测量与任何一个基本真实答案完全匹配的预测的百分比。EM 是 SQuAD 的主要指标之一。</p>
<p><strong>Mean Reciprocal Rank(MRP)</strong></p>
<p>在自然语言处理（NLP）任务中，MRR 经常被用来评估排序算法的性能，如查询文档排序和 QA。MRR 在式 4 中定义，其中 𝑄 是所有可能答案的集合，而 𝑟𝑎𝑛𝑘 是基本真理答案的排名位置。</p>
<script type="math/tex; mode=display">
MRP = \frac{1}{|Q|} \sum_{i=1}^Q \frac{1}{rank_i}</script><p>其他广泛使用的指标包括平均精度（MAP）、曲线下面积（AUC）、错误发现率、错误遗漏率等等。</p>
<p><strong>Quantitative Results</strong></p>
<p>我们将前面讨论的几种算法在流行的文本分类基准上的性能制成表格。在每一个表格中，除了一组有代表性的深度学习模型的结果外，我们还展示了使用非深度学习模型的结果，这些模型要么是以前最先进的，要么是在深度学习时代之前被广泛用作基线的。我们可以看到，在所有这些任务中，深度学习模型的使用带来了显著的改进。</p>
<p>在表 1 中对亚马逊数据集、Yelp 和 IMDB 中描述的几种模型的分析结果进行了总结。我们可以看到，自从第一个基于深度学习的情绪分析模型被引入以来，在准确度上有了一些显著的提高，例如，分类误差相对减少了 78%（在 SST-2 上）。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228162452.png" alt="image-20201228162451161"></p>
<p>表 2 报告了三个新闻分类数据集（即 AG 新闻、20 新闻、搜狗新闻）和两个主题分类数据集（即 DBpedia 和ohsummated）的性能。观察到与情绪分析类似的趋势。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228162527.png" alt="image-20201228162526197"></p>
<p>表 3 和表 4 分别展示了一些深度学习模型在 SQuAD 和 WikiQA 上的性能。值得注意的是，在这两个数据集中，显著的性能提升归因于 BERT 的使用。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228162541.png" alt="image-20201228162540492"></p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228162601.png" alt="image-20201228162600333"></p>
<p>表 5 给出了两个 NLI 数据集（即 SNLI 和 MNLI）的结果。我们观察到在过去的 5 年里，这两个数据集的性能都有了稳步的提高。</p>
<p><img src="https://gitee.com/xlshi/blog_img/raw/master/img/20201228162622.png" alt="image-20201228162621131"></p>
<h2 id="Challenges-and-Opportunities"><a href="#Challenges-and-Opportunities" class="headerlink" title="Challenges and Opportunities"></a>Challenges and Opportunities</h2><p>在基于深度学习的模型的帮助下，文本分类在过去的几年里取得了很大的进步。近十年来，人们提出了一些新的思想（如神经嵌入、注意机制、自我注意、变换器、BERT 和 XLNet）。尽管取得了所有进展，但我们面前仍有若干挑战需要解决。本节介绍了其中的一些挑战，并讨论了我们认为有助于推进该领域的研究方向。</p>
<h3 id="New-Datasets-for-More-Challenging-Tasks"><a href="#New-Datasets-for-More-Challenging-Tasks" class="headerlink" title="New Datasets for More Challenging Tasks"></a>New Datasets for More Challenging Tasks</h3><p>虽然近年来已经收集了大量用于文本分类的大规模数据集，但是对于更具挑战性的文本分类，仍然需要新的数据集来进行更具挑战性的任务，如多步推理的问答和多语种文档的文本分类。为这些任务建立一个大规模的标记数据集可以帮助加速这些领域的进展。</p>
<h3 id="Modeling-Commonsense-Knowledge"><a href="#Modeling-Commonsense-Knowledge" class="headerlink" title="Modeling Commonsense Knowledge"></a>Modeling Commonsense Knowledge</h3><p>将常识性知识纳入深度学习模型有可能显著提高模型性能，这与人类利用常识知识执行不同任务的方式非常相似。例如，装备有常识知识库的 QA 系统可以回答关于真实世界的问题。常识知识也有助于在信息不完整的情况下解决问题。利用人们对日常事物或概念的广泛持有的信念，人工智能系统可以像人们一样基于对未知事物的“默认”假设进行推理。尽管这一思想已经被用于情感分类，但还需要更多的研究来探索如何在神经模型中有效地建模和使用常识知识。</p>
<h3 id="Interpretable-Deep-Learning-Models"><a href="#Interpretable-Deep-Learning-Models" class="headerlink" title="Interpretable Deep Learning Models"></a>Interpretable Deep Learning Models</h3><p>虽然深度学习模型在具有挑战性的基准测试中取得了令人满意的性能，但这些模型中的大多数都是不可解释的，仍然存在许多悬而未决的问题。例如，为什么一个模型在一个数据集上优于另一个模型，而在其他数据集上却表现不佳？深度学习模型究竟学到了什么？什么是最小的神经网络架构，可以在给定的数据集上达到一定的精度？虽然注意力机制和自注意力机制对解答这些问题提供了一些见解，对这些模型的潜在行为和动力学的详细研究仍然缺乏。更好地理解这些模型的理论方面有助于开发更好的模型，以适应各种文本分析场景。</p>
<h3 id="Memory-Efficient-Models"><a href="#Memory-Efficient-Models" class="headerlink" title="Memory Efficient Models"></a>Memory Efficient Models</h3><p>大多数现代神经语言模型需要大量的记忆来进行训练和推理。但是为了满足移动设备的计算和存储限制，这些模型必须被简化和压缩。这可以通过使用知识蒸馏来构建学生模型，或者使用模型压缩技术来实现。开发一种任务无关的模型简化方法是一个活跃的研究课题。</p>
<h3 id="Few-Shot-and-Zero-Shot-Learning"><a href="#Few-Shot-and-Zero-Shot-Learning" class="headerlink" title="Few-Shot and Zero-Shot Learning"></a>Few-Shot and Zero-Shot Learning</h3><p>大多数深度学习模型都是有监督的模型，需要大量的域标签。实际上，为每个新域收集这样的标签是很昂贵的。将预先训练好的语言模型（PLM）（如 BERT 和 OpenGPT）微调到特定的任务需要比从头开始训练模型少得多的域标签，从而为基于 PLMS 开发新的零炮或少炮学习方法提供了机会。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>在这篇论文中，我们调查了超过 150 个深度学习模型，这些模型在过去的六年里发展起来，在不同的文本分类任务上，包括情感分析、新闻分类、主题分类、问答和 NLI，都有了显著的提高。我们还概述了 40 多个流行的文本分类数据集，并对这些模型在几个公共基准上的性能进行了定量分析。最后，我们讨论了一些开放的挑战和未来的研究方向。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-22T08:47:36.000Z" title="2020-12-22T08:47:36.000Z">2020-12-22</time><span class="level-item"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></span><span class="level-item">2 minutes read (About 344 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2066.%20%E6%9E%84%E5%BB%BA%E4%B9%98%E7%A7%AF%E6%95%B0%E7%BB%84/">LeetCode/剑指 Offer 66. 构建乘积数组</a></h1><div class="content"><h4 id="剑指-Offer-66-构建乘积数组"><a href="#剑指-Offer-66-构建乘积数组" class="headerlink" title="剑指 Offer 66. 构建乘积数组"></a><a href="https://leetcode-cn.com/problems/gou-jian-cheng-ji-shu-zu-lcof/">剑指 Offer 66. 构建乘积数组</a></h4><p>给定一个数组 A[0,1,…,n-1]，请构建一个数组 B[0,1,…,n-1]，其中 B 中的元素 B[i]=A[0]×A[1]×…×A[i-1]×A[i+1]×…×A[n-1]。不能使用除法。</p></div><a class="article-more button is-small size-small" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2066.%20%E6%9E%84%E5%BB%BA%E4%B9%98%E7%A7%AF%E6%95%B0%E7%BB%84/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-22T08:47:36.000Z" title="2020-12-22T08:47:36.000Z">2020-12-22</time><span class="level-item"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></span><span class="level-item">2 minutes read (About 318 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2065.%20%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E5%81%9A%E5%8A%A0%E6%B3%95/">LeetCode/剑指 Offer 65. 不用加减乘除做加法</a></h1><div class="content"><h4 id="剑指-Offer-65-不用加减乘除做加法"><a href="#剑指-Offer-65-不用加减乘除做加法" class="headerlink" title="剑指 Offer 65. 不用加减乘除做加法"></a><a href="https://leetcode-cn.com/problems/qiu-12n-lcof/"><a href="https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/">剑指 Offer 65. 不用加减乘除做加法</a></a></h4><p>写一个函数，求两个整数之和，要求在函数体内不得使用 “+”、“-”、“*”、“/” 四则运算符号。</p></div><a class="article-more button is-small size-small" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2065.%20%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E5%81%9A%E5%8A%A0%E6%B3%95/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-22T08:47:36.000Z" title="2020-12-22T08:47:36.000Z">2020-12-22</time><span class="level-item"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></span><span class="level-item">2 minutes read (About 247 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2064.%20%E6%B1%821+2+%E2%80%A6+n/">LeetCode/剑指 Offer 64. 求1+2+…+n</a></h1><div class="content"><h4 id="剑指-Offer-64-求1-2-…-n"><a href="#剑指-Offer-64-求1-2-…-n" class="headerlink" title="剑指 Offer 64. 求1+2+…+n"></a><a href="https://leetcode-cn.com/problems/qiu-12n-lcof/">剑指 Offer 64. 求1+2+…+n</a></h4><p>求 <code>1+2+...+n</code> ，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。</p></div><a class="article-more button is-small size-small" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2064.%20%E6%B1%821+2+%E2%80%A6+n/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-22T08:47:36.000Z" title="2020-12-22T08:47:36.000Z">2020-12-22</time><span class="level-item"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></span><span class="level-item">3 minutes read (About 387 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2063.%20%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E5%A4%A7%E5%88%A9%E6%B6%A6/">LeetCode/剑指 Offer 63. 股票的最大利润</a></h1><div class="content"><h4 id="剑指-Offer-63-股票的最大利润"><a href="#剑指-Offer-63-股票的最大利润" class="headerlink" title="剑指 Offer 63. 股票的最大利润"></a><a href="https://leetcode-cn.com/problems/gu-piao-de-zui-da-li-run-lcof/">剑指 Offer 63. 股票的最大利润</a></h4><p>假设把某股票的价格按照时间先后顺序存储在数组中，请问买卖该股票一次可能获得的最大利润是多少？</p></div><a class="article-more button is-small size-small" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2063.%20%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E5%A4%A7%E5%88%A9%E6%B6%A6/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-22T08:47:36.000Z" title="2020-12-22T08:47:36.000Z">2020-12-22</time><span class="level-item"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></span><span class="level-item">2 minutes read (About 239 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/LeetCode/387.%20%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%94%AF%E4%B8%80%E5%AD%97%E7%AC%A6/">LeetCode/387. 字符串中的第一个唯一字符</a></h1><div class="content"><h4 id="387-字符串中的第一个唯一字符"><a href="#387-字符串中的第一个唯一字符" class="headerlink" title="387. 字符串中的第一个唯一字符"></a><a href="https://leetcode-cn.com/problems/first-unique-character-in-a-string/">387. 字符串中的第一个唯一字符</a></h4><p>给定一个字符串，找到它的第一个不重复的字符，并返回它的索引。如果不存在，则返回 -1。</p></div><a class="article-more button is-small size-small" href="/2020/12/22/LeetCode/387.%20%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%94%AF%E4%B8%80%E5%AD%97%E7%AC%A6/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-22T08:47:36.000Z" title="2020-12-22T08:47:36.000Z">2020-12-22</time><span class="level-item"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></span><span class="level-item">3 minutes read (About 434 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2061.%20%E6%89%91%E5%85%8B%E7%89%8C%E4%B8%AD%E7%9A%84%E9%A1%BA%E5%AD%90/">LeetCode/剑指 Offer 61. 扑克牌中的顺子</a></h1><div class="content"><h4 id="剑指-Offer-61-扑克牌中的顺子"><a href="#剑指-Offer-61-扑克牌中的顺子" class="headerlink" title="剑指 Offer 61. 扑克牌中的顺子"></a><a href="https://leetcode-cn.com/problems/bu-ke-pai-zhong-de-shun-zi-lcof/">剑指 Offer 61. 扑克牌中的顺子</a></h4><p>从扑克牌中随机抽5张牌，判断是不是一个顺子，即这5张牌是不是连续的。2～10为数字本身，A为1，J为11，Q为12，K为13，而大、小王为 0 ，可以看成任意数字。A 不能视为 14。</p></div><a class="article-more button is-small size-small" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2061.%20%E6%89%91%E5%85%8B%E7%89%8C%E4%B8%AD%E7%9A%84%E9%A1%BA%E5%AD%90/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-22T07:43:48.000Z" title="2020-12-22T07:43:48.000Z">2020-12-22</time><span class="level-item"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></span><span class="level-item">3 minutes read (About 476 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2059%20-%20II.%20%E9%98%9F%E5%88%97%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/">LeetCode/剑指 Offer 59 - II. 队列的最大值</a></h1><div class="content"><h4 id="剑指-Offer-59-II-队列的最大值"><a href="#剑指-Offer-59-II-队列的最大值" class="headerlink" title="剑指 Offer 59 - II. 队列的最大值"></a><a href="https://leetcode-cn.com/problems/dui-lie-de-zui-da-zhi-lcof/">剑指 Offer 59 - II. 队列的最大值</a></h4><p>请定义一个队列并实现函数 <code>max_value</code> 得到队列里的最大值，要求函数<code>max_value</code>、<code>push_back</code> 和 <code>pop_front</code> 的均摊时间复杂度都是<code>O(1)</code>。</p>
<p>若队列为空，<code>pop_front</code> 和 <code>max_value</code> 需要返回 <code>-1</code></p></div><a class="article-more button is-small size-small" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2059%20-%20II.%20%E9%98%9F%E5%88%97%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-12-22T06:37:38.000Z" title="2020-12-22T06:37:38.000Z">2020-12-22</time><span class="level-item"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></span><span class="level-item">4 minutes read (About 671 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2059%20-%20I.%20%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/">LeetCode/剑指 Offer 59 - I. 滑动窗口的最大值</a></h1><div class="content"><h4 id="剑指-Offer-59-I-滑动窗口的最大值"><a href="#剑指-Offer-59-I-滑动窗口的最大值" class="headerlink" title="剑指 Offer 59 - I. 滑动窗口的最大值"></a><a href="https://leetcode-cn.com/problems/hua-dong-chuang-kou-de-zui-da-zhi-lcof/">剑指 Offer 59 - I. 滑动窗口的最大值</a></h4><p>给定一个数组 <code>nums</code> 和滑动窗口的大小 <code>k</code>，请找出所有滑动窗口里的最大值。</p></div><a class="article-more button is-small size-small" href="/2020/12/22/LeetCode/%E5%89%91%E6%8C%87%20Offer%2059%20-%20I.%20%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/#more">Read More</a></article></div><nav class="pagination is-centered mt-4" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/4/">Previous</a></div><div class="pagination-next"><a href="/page/6/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/4/">4</a></li><li><a class="pagination-link is-current" href="/page/5/">5</a></li><li><a class="pagination-link" href="/page/6/">6</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/31/">31</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="https://s1.ax1x.com/2020/05/04/Y9rl0U.jpg" alt="是龙弟弟呀"></figure><p class="title is-size-4 is-block line-height-inherit">是龙弟弟呀</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Xi&#039;an,Shaanxi</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">306</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">27</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">54</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/sxlong0205" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/sxlong0205"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Algorithm/%E7%AE%97%E6%B3%954%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">算法4笔记</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/IDEA%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"><span class="level-start"><span class="level-item">IDEA使用技巧</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/IDEA%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/%E9%A1%B9%E7%9B%AE%E7%83%AD%E9%83%A8%E7%BD%B2/"><span class="level-start"><span class="level-item">项目热部署</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Java%E6%A1%86%E6%9E%B6/"><span class="level-start"><span class="level-item">Java框架</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Java%E6%A1%86%E6%9E%B6/MyBatis/"><span class="level-start"><span class="level-item">MyBatis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Java%E8%99%9A%E6%8B%9F%E6%9C%BA/"><span class="level-start"><span class="level-item">Java虚拟机</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LeetCode/"><span class="level-start"><span class="level-item">LeetCode</span></span><span class="level-end"><span class="level-item tag">238</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">8</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Linux/Git%E5%AE%89%E8%A3%85/"><span class="level-start"><span class="level-item">Git安装</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/MySQL%E5%AE%89%E8%A3%85/"><span class="level-start"><span class="level-item">MySQL安装</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/Nginx%E5%AE%89%E8%A3%85/"><span class="level-start"><span class="level-item">Nginx安装</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/%E5%AE%89%E8%A3%85Maven/"><span class="level-start"><span class="level-item">安装Maven</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/%E5%AE%89%E8%A3%85Oracle-JDK/"><span class="level-start"><span class="level-item">安装Oracle JDK</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/%E5%AE%89%E8%A3%85Tomcat/"><span class="level-start"><span class="level-item">安装Tomcat</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/"><span class="level-start"><span class="level-item">虚拟机配置</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Spring%E6%A1%86%E6%9E%B6/"><span class="level-start"><span class="level-item">Spring框架</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Spring%E6%A1%86%E6%9E%B6/SSM%E6%95%B4%E5%90%88/"><span class="level-start"><span class="level-item">SSM整合</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"><span class="level-start"><span class="level-item">并发编程</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="level-start"><span class="level-item">设计模式</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="level-start"><span class="level-item">数据库</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"><span class="level-start"><span class="level-item">MySQL</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="level-start"><span class="level-item">服务器</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/Tomcat/"><span class="level-start"><span class="level-item">Tomcat</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">19</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="level-start"><span class="level-item">图解机器学习笔记</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li></ul></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Class%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/"><span class="tag">Class文件结构</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JDBC/"><span class="tag">JDBC</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JDK/"><span class="tag">JDK</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JVM%E4%B8%8EJava%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"><span class="tag">JVM与Java体系结构</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Maven/"><span class="tag">Maven</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MyBatis%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">MyBatis官方文档学习笔记</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MySQL/"><span class="tag">MySQL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nginx/"><span class="tag">Nginx</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SSM%E6%95%B4%E5%90%88/"><span class="tag">SSM整合</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/StringTable/"><span class="tag">StringTable</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tomcat/"><span class="tag">Tomcat</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%B1%E7%A0%81/"><span class="tag">乱码</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%86%8D%E8%B0%88%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%99%A8/"><span class="tag">再谈类的加载器</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"><span class="tag">刷题笔记</span><span class="tag is-grey-lightest">238</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"><span class="tag">半监督学习</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0/"><span class="tag">在线学习</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8/"><span class="tag">垃圾回收器</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%A6%82%E8%BF%B0/"><span class="tag">垃圾回收概述</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/"><span class="tag">垃圾回收相关概念</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/"><span class="tag">垃圾回收相关算法</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A0%86/"><span class="tag">堆</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/"><span class="tag">多任务学习</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E8%8A%82%E7%A0%81%E6%8C%87%E4%BB%A4%E9%9B%86%E4%B8%8E%E8%A7%A3%E6%9E%90%E4%B8%BE%E4%BE%8B/"><span class="tag">字节码指令集与解析举例</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"><span class="tag">学习模型</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80%E4%B8%8E%E8%AE%BF%E9%97%AE%E5%AE%9A%E4%BD%8D/"><span class="tag">对象实例化内存布局与访问定位</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B8%A6%E7%BA%A6%E6%9D%9F/"><span class="tag">带约束</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BA%8F%E5%88%97%E5%88%86%E7%B1%BB/"><span class="tag">序列分类</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"><span class="tag">异常检测</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E/"><span class="tag">执行引擎</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"><span class="tag">排序算法</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"><span class="tag">支持向量机</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%B9%E6%B3%95%E5%8C%BA/"><span class="tag">方法区</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E9%99%8D%E7%BB%B4/"><span class="tag">无监督降维</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%88%86%E7%B1%BB/"><span class="tag">最小二乘分类</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"><span class="tag">最小二乘法</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%AC%E5%9C%B0%E6%96%B9%E6%B3%95%E5%B0%86%E6%8E%A5%E5%8F%A3/"><span class="tag">本地方法将接口</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%AC%E5%9C%B0%E6%96%B9%E6%B3%95%E6%A0%88/"><span class="tag">本地方法栈</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A6%82%E7%8E%87%E5%88%86%E7%B1%BB%E6%B3%95/"><span class="tag">概率分类法</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%83%AD%E9%83%A8%E7%BD%B2/"><span class="tag">热部署</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%9B%91%E7%9D%A3%E9%99%8D%E7%BB%B4/"><span class="tag">监督降维</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98/"><span class="tag">直接内存</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/"><span class="tag">稀疏学习</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A8%8B%E5%BA%8F%E8%AE%A1%E6%95%B0%E5%99%A8/"><span class="tag">程序计数器</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/"><span class="tag">类加载子系统</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/"><span class="tag">类的加载过程详解</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%81%9A%E7%B1%BB/"><span class="tag">聚类</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A0%88/"><span class="tag">虚拟机栈</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="tag">迁移学习</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E6%A6%82%E8%BF%B0%E5%8F%8A%E7%BA%BF%E7%A8%8B/"><span class="tag">运行时数据区概述及线程</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81IP/"><span class="tag">配置静态IP</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9B%86%E6%88%90%E5%88%86%E7%B1%BB/"><span class="tag">集成分类</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%B2%81%E6%A3%92%E5%AD%A6%E4%B9%A0/"><span class="tag">鲁棒学习</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-12-29T09:47:38.000Z">2020-12-29</time></p><p class="title is-6"><a class="link-muted" href="/2020/12/29/%E8%AE%BA%E6%96%87/Attention%20Is%20All%20You%20Need/">论文/Attention Is All You Need</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-12-29T04:24:41.000Z">2020-12-29</time></p><p class="title is-6"><a class="link-muted" href="/2020/12/29/LeetCode/236.%20%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/">LeetCode/236. 二叉树的最近公共祖先</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-12-29T04:23:55.000Z">2020-12-29</time></p><p class="title is-6"><a class="link-muted" href="/2020/12/29/LeetCode/%E5%89%91%E6%8C%87%20Offer%2068%20-%20II.%20%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/">LeetCode/剑指 Offer 68 - II. 二叉树的最近公共祖先</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-12-29T03:53:41.000Z">2020-12-29</time></p><p class="title is-6"><a class="link-muted" href="/2020/12/29/LeetCode/235.%20%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/">LeetCode/235. 二叉搜索树的最近公共祖先</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-12-29T03:53:11.000Z">2020-12-29</time></p><p class="title is-6"><a class="link-muted" href="/2020/12/29/LeetCode/%E5%89%91%E6%8C%87%20Offer%2068%20-%20I.%20%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/">LeetCode/剑指 Offer 68 - I. 二叉搜索树的最近公共祖先</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/LeetCode/">LeetCode</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">53</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">63</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">52</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">70</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Code Dragon" height="28"></a><p class="size-small"><span>&copy; 2021 Code Dragon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://sxlong0205.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>